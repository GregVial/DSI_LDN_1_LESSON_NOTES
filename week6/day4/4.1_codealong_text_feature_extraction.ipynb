{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Extraction from Text (Natural Language Processing)\n",
    "Week 6| Lesson 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Extract features from free form text using Scikit Learn\n",
    "- Identify Parts of Speech using NLTK\n",
    "- Remove stop words\n",
    "- Describe how TF-IDF works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Familiarize yourself with [nltk.download()](http://www.nltk.org/data.html) in case you need to download additional corpuses\n",
    "- Describe what a transformer is in Scikit Learn and use it\n",
    "- Recognize basic principles of English language syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best not to run this cell as it will occupy your notebook kernel,\n",
    "# better to take this and run it in a terminal\n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening\n",
    "All the models we have learned so far accept a 2D table of real numbers as input (we called it X) and output a vector of classes or numbers (we called it y). Very often though, our starting point data is not given in the form of a table of numbers, but it's unstructured. We have seen how we have to deal with structuring data in the case of scraping, for example. Text documents are a common case in point where we must define and formulate the structure we want to see. In this case we need a way to go from unstructure data to a table of numbers, so that we can then apply the usual methods. This is called _feature extraction_.\n",
    "\n",
    "**Check:** Take a couple of minutes to think of real-world applications of Natural Language Processing and discuss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Feature Extraction from Text\n",
    "\n",
    "### A simple example\n",
    "Suppose we are building a spam/ham classifier. Input are emails, output is a binary classification.\n",
    "\n",
    "Here's an example of an input email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "--\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\"\n",
    "print(spam)\n",
    "print(\"--\")\n",
    "print (ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** can you think of a simple heuristic rule to catch email like this (meaning an imperfect but rapid solution)?\n",
    "\n",
    "By defining a simple rule that parses the text we have performed one of the simplest feature extraction from text: binary word counting. \n",
    "\n",
    "### Bag of words (word counting)\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the count of its words, disregarding grammar and even word order (and hence generally much meaning, but topics in particular are well represented by this approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'is': 2, 'etc.': 2, 'am': 2, 'an': 2, 'have': 2, 'in': 2, 'your': 2, 'to': 2, 'years': 2, 'with': 2, 'this': 2, 'contact': 2, 'the': 2, 'major': 1, 'old': 1, 'cancer': 1, 'outstanding': 1, 'seven': 1, 'decided': 1, 'through': 1, 'carefully': 1, 'euros(eight': 1, 'seem': 1, 'saw': 1, 'information': 1, 'for': 1, 'euros': 1, 'fifty': 1, '86': 1, 'sum': 1, '\"lukoil\".': 1, 'only': 1, 'pjsc': 1, 'mr.': 1, '2': 1, 'linkedin.': 1, 'will/donate': 1, 'you': 1, 'hundred': 1, 'was': 1, 'personality.': 1, 'chairman': 1, 'profile': 1, 'you.': 1, 'hello,': 1, 'ago.': 1, 'read': 1, 'going': 1, 'thousand': 1, 'million': 1, 'grayfer': 1, 'reason': 1, 'be': 1, 'one': 1, 'why': 1, 'on': 1, 'name': 1, 'week.': 1, '8,750,000.00': 1, 'later': 1, 'board': 1, 'operation': 1, 'will': 1, 'directors': 1, 'diagnosed': 1, 'valery': 1, 'my': 1})\n",
      "--\n",
      "Counter({'to': 5, 'you': 4, 'of': 4, 'the': 3, 'and': 2, 'we': 2, 'scientist': 2, 'data': 2, 'i': 2, 'further': 2, 'this': 1, 'regards.': 1, 'find': 1, 'information': 1, 'am': 1, 'an': 1, 'at': 1, 'in': 1, 'our': 1, 'message': 1, 'pleased': 1, 'best': 1, 'if': 1, 'will': 1, 'would': 1, 'with': 1, 'interviews': 1, 'please': 1, 'writing': 1, 'application': 1, 'mr.': 1, 'location': 1, 'passed': 1, 'interview': 1, 'for': 1, 'john': 1, 'date,': 1, 'be': 1, 'hello,': 1, 'x.': 1, 'invite': 1, 'that': 1, 'any': 1, 'interview.': 1, 'regards': 1, 'let': 1, 'know': 1, 'hooli': 1, 'on-site': 1, 'me': 1, 'on': 1, 'your': 1, 'like': 1, 'assistance.': 1, 'attached': 1, 'senior': 1, 'inform': 1, 'smith.': 1, 'can': 1, 'time': 1, 'position': 1, 'first': 1, 'round': 1, 'are': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(spam.lower().split()))\n",
    "print(\"--\")\n",
    "print(Counter(ham.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we counted the number of times each word appeared in the text. Note that since we included all the words in the text, we created a dictionary that contains many words with only one appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Scikit Learn Count Vectorizer\n",
    "\n",
    "Scikit learn offers a way to do bag-of-words with the CountVectorizer, with many configurable options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise and fit to the 'spam' message we defined above\n",
    "# Note we set n_gram to (1,1) as default so we only return single words\n",
    "# Note we did not define stop words so these are included in the fit\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cancer',\n",
       " u'carefully',\n",
       " u'chairman',\n",
       " u'contact',\n",
       " u'decided',\n",
       " u'diagnosed',\n",
       " u'directors',\n",
       " u'donate',\n",
       " u'eight',\n",
       " u'etc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns features (just showing ten out of the total of 69)\n",
    "# Note that all words have been set to lowercase, this is a default option (see output above)\n",
    "\n",
    "cvec.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did this operation do? Remember we follow a fit and transform flow with sklearn. So first we fit the\n",
    "CountVectorizer object - this means we have defined the dictionary of possible words. Next if we transform based\n",
    "on any document we input, it will count the number of times words in that dictionary appear in the inputted document (we usually use the word document to mean any individual text input, which could for example be rows in a dataframe - each row is then a document). \n",
    "\n",
    "When we then do a transform, if a word\n",
    "in a text document input to the fitted CountVectorizer was not present in the original document we based our fit then\n",
    "it simply will not appear in the transformed output at all (i.e. there is no base corpus of words, you define that when you perform the fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "of         4\n",
       "and        3\n",
       "your       2\n",
       "this       2\n",
       "in         2\n",
       "you        2\n",
       "have       2\n",
       "euros      2\n",
       "etc        2\n",
       "contact    2\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(cvec.transform([spam]).todense(), columns=cvec.get_feature_names())\n",
    "df.transpose()[0].sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can choose several parameters to tweak.\n",
    "\n",
    "**Check:** spend a couple of minutes scanning the documentation to figure out what those parameters do. Take 5 minutes, then share a few takeaways from the documentation in groups. What features stand out to you?\n",
    "\n",
    "[CountVectorizer Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided_practice\"></a>\n",
    "## Sklearn HashingVectorizer\n",
    "\n",
    "As you have seen in the CountVectorizer documentation we can set the dictionary to have a fixed size, or only keeping words of certain frequencies, however we still have to compute a dictionary and hold the dictionary in memory. This could be a problem when we have a large corpus.\n",
    "\n",
    "One approach to optimising this in sklearn known as the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences. Each word is mapped to a feature with the use of a hash function that converts it to a hash (essentially an index). If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. We do not actually fit the hashing function, it has already been defined. So we can use the hashing approach for words we have never actually seen before. This is very convenient however the main drawback of the this is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. It's also possible (though unlikely) for two words to be assigned the same hash and thus for the accuracy of a model built on the HashingVectorizer to underperform than that built with a CountVectorizer (hash collision). However, the memory optimisation is substantial decreasing both the run time and memory allocation and the impact on accuracy will tend to be minimal owing to the number of possible hashes, but of course it pays to verify when building and testing. I have included a paper which describes the methodology [here](./assets/papers/feature_hashing.pdf). There are 2^20 possible hashes in the default in sklearn, over 1 million, so clashes are not likely (you can set this to a maximum of 2^31 - 1).\n",
    "\n",
    "So the key to remember here is that from the modelling perspective, we don't actually input words. We just input some value (index, hash) corresponding to a word and its frequency, and we get out whether that was a predictive feature. But it doesn't matter what word it was, unless we want to actually interpret our model. But interpretation is distinct from accurate prediction. So, generally you would perform exploration using CountVectorizer in order to be able to readily query your model, and then implement HashingVectorizer in production in order to reduce memory load. This is particularly a concern in text applications because of the sheer size of the potential number of features when you start including multiple combinations of words (ngrams).\n",
    "\n",
    "\n",
    "**Check:** What characteristics should feature extraction from text satisfy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HashingVectorizer is implemented in the same way but there\n",
    "# is no need to fit this time, since the hashing function\n",
    "# has already been defined by sklearn. If you run fit, \n",
    "# it will just do nothing\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hvec = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479532     0.338062\n",
       "180525     0.253546\n",
       "1005907    0.169031\n",
       "994433     0.169031\n",
       "170062     0.169031\n",
       "174171     0.169031\n",
       "832412     0.169031\n",
       "967636     0.169031\n",
       "757616     0.169031\n",
       "144749     0.169031\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output has been scaled and interpretation is lost to hash values instead of words.\n",
    "# Additionally even the scaled values do not directly correspond to counts as values\n",
    "# can be set to be negative (which is a strategy to avoid hash collisions).\n",
    "# See for example recent developments on this implementation in sklearn here\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/7513 \n",
    "\n",
    "df2  = pd.DataFrame(hvec.transform([spam]).todense())\n",
    "df2.transpose()[0].abs().sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** What new parameters does this vectoriser offer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction_2\"></a>\n",
    "## Intro: NLTK (Natural Language Tool Kit)\n",
    "\n",
    "Bag of word approaches like the one outlined before completely ignore the structure of a sentence, they merely assess presence of specific words or word combinations. Here are some additional techniques that can help to build up the complexity required to deal with language as it is actually used (bear in mind dealing with natural language includes some of the toughest problems in machine learning today).\n",
    "\n",
    "### Segmentation\n",
    "\n",
    "_Segmentation_ is a technique to identify (e.g.) sentences within a body of text. Punctuation serves as our guide in the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\", \"What do you think of that?\", \"I bet you hate it!\", \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_sentencer(text):\n",
    "    '''Take a string `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "    \n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentencer above doesn't work perfectly. In the lab you will learn how to improve it. Thankfully over many years people have worked on such issues and the NLTK library offers an easy to use sentencer (thank you open source coding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You have to use nltk.download() to get the PunktSentenceTokenizer\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** Does NLTK offer other Tokenizers? Use nltk.download() to explore the available packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo_2\"></a>\n",
    "### Stemming\n",
    "\n",
    "Normalisation is when slightly different version of a word exist. For example: LinkedIn will see hundreds of variations of the title \"Software Developer\" (including \"Code Ninja\").\n",
    "\n",
    "**Check:** What are other common cases of text that could need normalisation?\n",
    "\n",
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalise words to a common root. This technique is called _stemming_.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim\n",
    "\n",
    "As we did above we could define a Stemmer based on rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scien', 'Scien']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, NLTK contains several robust stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swim\n",
      "Swim\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('Swimmed'))\n",
    "print(stemmer.stem('Swimming'))\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** There are various stemmers available in NLTK, such as the additional two importered above. Take a look at [the documentation](http://www.nltk.org/api/nltk.stem.html) and see if you can find the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Some words are very common and provide no information on the text content. We should remove these.\n",
    "\n",
    "**Check:** Can you give some examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'interesting', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# You will need to have used nltk.download() to download the stopwords corpus for this to work\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "sentence = \"this is a really interesting sentence\"\n",
    "print([i for i in sentence.split() if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note you might want to add or remove words from the stopwords list, depending\n",
    "# on your application (e.g. domain specific words)\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "Each word has a specific role in a sentence (Verb, Noun etc.) Parts-of-speech tagging (POS) is a feature extraction technique that attaches a tag to each word in the sentence, in order to provide a more precise context for further analysis. This is often a resource intensive process, but it can sometimes improve the accuracy or our models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('nlp', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize separates the words nad pos_tag identifies the word type\n",
    "# there are various taggers available, take a look at nltk.tag documentation\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "pos_tag(word_tokenize(\"today is a great day to learn nlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# To find these tags and their meaning. How did I know it was the upenn tagset? \n",
    "# I'm afraid NLTK documentation is not so good and I had to go digging in the source code to confirm this. \n",
    "# So probably a bit of code you want to save if you're looking to interpret these tags.\n",
    "\n",
    "import nltk.help as help_nltk\n",
    "help_nltk.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice_2\"></a>\n",
    "# Term frequency - Inverse document Frequency\n",
    "\n",
    "More powerful than just removing known stop-words is the TF-IDF approach. This tells us which words are most discriminative between documents. Words that occur a lot in one document but don't occur in many documents will tell you something special about the document. Of course, this can be used to innately reduce the importance of stop words as well as more powerfully generalising for domain specific terms. Its calculation is straightforward.\n",
    "\n",
    "Term frequency TF is the frequency of a certain term (e.g. a word or stem) in a document:\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = \\frac{N_\\text{term}}{N_\\text{terms in Document}}\n",
    "$$\n",
    "\n",
    "Inverse document frequency IDF of a word is defined as the number of documents in the corpus divided by the number of documents that contain the term - typically a log scaling is used (this is the implementation in sklearn).\n",
    "$$\n",
    "\\mathrm{idf}(t, D) = \\log(\\frac{1+ N_\\text{Documents}}{1+ N_\\text{Documents that contain term}} +1 )\n",
    "$$\n",
    "\n",
    "Term frequency Inverse Document Frequency (TF-IDF) is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "This enhances terms that are highly specific of a particular document, while suppressing terms that are common to most documents. Hence, if a term has a high TFIDF in a particular document it means that it occurs proportionately more in that particular document than it tends to occur in the corpus as a whole (all documents). Hence it seems to be an interesting word from the perspective of identifying characteristics of the document - such as the topic (eg is the document about politics, mathematics, the environment?), or the sentiment (eg is the writer expressing happiness, sadness, or frustration?).\n",
    "\n",
    "Scikit Learn introduces a TFIDF vectorizer that works similarly to the other vectorizers. Alternatively, we could use CountVectorizer followed by TfidfTransformer. We want to be careful about what our corpus will be to interpret this well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a spam dataset with labels\n",
    "\n",
    "corpus=pd.read_csv(\"assets/SMSSpamCollection.txt\", sep=\"\\t\", header=None, encoding=\"utf-8\")\n",
    "corpus.columns=[\"classification\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's initialise the tfidf and calculate the word frequencies for the whole corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec.fit(corpus[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we take single document examples at random to compare their contents to the corpus\n",
    "example_spam=corpus.loc[corpus[\"classification\"]==\"spam\",:].iloc[100]\n",
    "example_ham=corpus.loc[corpus[\"classification\"]==\"ham\",:].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'To review and KEEP the fantastic Nokia N-Gage game deck with Club Nokia, go 2 www.cnupdates.com/newsletter. unsubscribe from alerts reply with the word OUT'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_spam[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Hmm...my uncle just informed me that he's paying the school directly. So pls buy food.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ham[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This performs the tfidf transformation on the two examples and puts them in a dataframe\n",
    "# so we can compare the word frequency in each document with the word frequencies in the whole\n",
    "# corpus, and identify words which stand out\n",
    "\n",
    "df3  = pd.DataFrame(tvec.transform([example_spam[\"text\"], example_ham[\"text\"]]).todense(), columns=tvec.get_feature_names(), index=['example_spam', 'example_ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nokia</th>\n",
       "      <th>deck</th>\n",
       "      <th>newsletter</th>\n",
       "      <th>cnupdates</th>\n",
       "      <th>gage</th>\n",
       "      <th>alerts</th>\n",
       "      <th>review</th>\n",
       "      <th>with</th>\n",
       "      <th>fantastic</th>\n",
       "      <th>club</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_spam</th>\n",
       "      <td>0.346322</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.235925</td>\n",
       "      <td>0.231317</td>\n",
       "      <td>0.222039</td>\n",
       "      <td>0.20691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 nokia     deck  newsletter  cnupdates     gage   alerts  \\\n",
       "example_spam  0.346322  0.27441     0.27441    0.27441  0.27441  0.27441   \n",
       "example_ham   0.000000  0.00000     0.00000    0.00000  0.00000  0.00000   \n",
       "\n",
       "                review      with  fantastic     club  \n",
       "example_spam  0.235925  0.231317   0.222039  0.20691  \n",
       "example_ham   0.000000  0.000000   0.000000  0.00000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we display the columns sorted by the spam example's highest ranking tfidf score for the top 10.\n",
    "df3.transpose().sort_values('example_spam', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directly</th>\n",
       "      <th>informed</th>\n",
       "      <th>paying</th>\n",
       "      <th>uncle</th>\n",
       "      <th>hmm</th>\n",
       "      <th>food</th>\n",
       "      <th>school</th>\n",
       "      <th>buy</th>\n",
       "      <th>pls</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_spam</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_ham</th>\n",
       "      <td>0.345969</td>\n",
       "      <td>0.345969</td>\n",
       "      <td>0.334647</td>\n",
       "      <td>0.318081</td>\n",
       "      <td>0.311635</td>\n",
       "      <td>0.292379</td>\n",
       "      <td>0.286856</td>\n",
       "      <td>0.246985</td>\n",
       "      <td>0.221469</td>\n",
       "      <td>0.20174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              directly  informed    paying     uncle       hmm      food  \\\n",
       "example_spam  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "example_ham   0.345969  0.345969  0.334647  0.318081  0.311635  0.292379   \n",
       "\n",
       "                school       buy       pls       he  \n",
       "example_spam  0.000000  0.000000  0.000000  0.00000  \n",
       "example_ham   0.286856  0.246985  0.221469  0.20174  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows us similarly the ham example's highest ranking tfidf scores\n",
    "df3.transpose().sort_values('example_ham', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alerts</th>\n",
       "      <th>and</th>\n",
       "      <th>buy</th>\n",
       "      <th>club</th>\n",
       "      <th>cnupdates</th>\n",
       "      <th>com</th>\n",
       "      <th>deck</th>\n",
       "      <th>directly</th>\n",
       "      <th>fantastic</th>\n",
       "      <th>food</th>\n",
       "      <th>from</th>\n",
       "      <th>gage</th>\n",
       "      <th>game</th>\n",
       "      <th>go</th>\n",
       "      <th>he</th>\n",
       "      <th>hmm</th>\n",
       "      <th>informed</th>\n",
       "      <th>just</th>\n",
       "      <th>keep</th>\n",
       "      <th>me</th>\n",
       "      <th>my</th>\n",
       "      <th>newsletter</th>\n",
       "      <th>nokia</th>\n",
       "      <th>out</th>\n",
       "      <th>paying</th>\n",
       "      <th>pls</th>\n",
       "      <th>reply</th>\n",
       "      <th>review</th>\n",
       "      <th>school</th>\n",
       "      <th>so</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>uncle</th>\n",
       "      <th>unsubscribe</th>\n",
       "      <th>with</th>\n",
       "      <th>word</th>\n",
       "      <th>www</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_spam</th>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.090505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20691</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.165189</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123267</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.202175</td>\n",
       "      <td>0.124293</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27441</td>\n",
       "      <td>0.346322</td>\n",
       "      <td>0.124526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144785</td>\n",
       "      <td>0.235925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.164818</td>\n",
       "      <td>0.067412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20525</td>\n",
       "      <td>0.231317</td>\n",
       "      <td>0.183158</td>\n",
       "      <td>0.153926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_ham</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.345969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20174</td>\n",
       "      <td>0.311635</td>\n",
       "      <td>0.345969</td>\n",
       "      <td>0.167721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139092</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334647</td>\n",
       "      <td>0.221469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286856</td>\n",
       "      <td>0.160044</td>\n",
       "      <td>0.15251</td>\n",
       "      <td>0.120848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318081</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               alerts       and       buy     club  cnupdates       com  \\\n",
       "example_spam  0.27441  0.090505  0.000000  0.20691    0.27441  0.165189   \n",
       "example_ham   0.00000  0.000000  0.246985  0.00000    0.00000  0.000000   \n",
       "\n",
       "                 deck  directly  fantastic      food      from     gage  \\\n",
       "example_spam  0.27441  0.000000   0.222039  0.000000  0.123267  0.27441   \n",
       "example_ham   0.00000  0.345969   0.000000  0.292379  0.000000  0.00000   \n",
       "\n",
       "                  game        go       he       hmm  informed      just  \\\n",
       "example_spam  0.202175  0.124293  0.00000  0.000000  0.000000  0.000000   \n",
       "example_ham   0.000000  0.000000  0.20174  0.311635  0.345969  0.167721   \n",
       "\n",
       "                  keep        me        my  newsletter     nokia       out  \\\n",
       "example_spam  0.166534  0.000000  0.000000     0.27441  0.346322  0.124526   \n",
       "example_ham   0.000000  0.139092  0.144414     0.00000  0.000000  0.000000   \n",
       "\n",
       "                paying       pls     reply    review    school        so  \\\n",
       "example_spam  0.000000  0.000000  0.144785  0.235925  0.000000  0.000000   \n",
       "example_ham   0.334647  0.221469  0.000000  0.000000  0.286856  0.160044   \n",
       "\n",
       "                 that       the        to     uncle  unsubscribe      with  \\\n",
       "example_spam  0.00000  0.164818  0.067412  0.000000      0.20525  0.231317   \n",
       "example_ham   0.15251  0.120848  0.000000  0.318081      0.00000  0.000000   \n",
       "\n",
       "                  word       www  \n",
       "example_spam  0.183158  0.153926  \n",
       "example_ham   0.000000  0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows us all the word features that are actually present in either of the examples and their tfidf\n",
    "# just as a useful piece of code\n",
    "df3.loc[:, (df3 != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.964125560538\n",
      "[[966   0]\n",
      " [ 40 109]]\n",
      "remember the distribution is uneven at:\n",
      "0    966\n",
      "1    149\n",
      "Name: classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Just to walk through the syntax, here's one way you could do a basic spam filter with TFIDF\n",
    "\n",
    "# Preparing the data: feature selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus[\"text\"], corpus[\"classification\"], test_size=0.2, random_state=42)\n",
    "mapping={\"spam\":1, \"ham\":0}\n",
    "y_train=y_train.map(mapping)\n",
    "y_test=y_test.map(mapping)\n",
    "X_train_tfidf=pd.DataFrame(tvec.transform(X_train).todense(), columns=tvec.get_feature_names())\n",
    "X_test_tfidf=pd.DataFrame(tvec.transform(X_test).todense(), columns=tvec.get_feature_names())\n",
    "\n",
    "# Modelling based on those features and assess model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "logit=LogisticRegression()\n",
    "logit.fit(X_train_tfidf, y_train)\n",
    "predictions=logit.predict(X_test_tfidf)\n",
    "print(\"accuracy is\", accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"remember the distribution is uneven at:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** What do we do all this for? Remember we started with just unstructured text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "In this lesson we learned about the first steps in Natural Language Processing and about two very powerful toolkits:\n",
    "- Scikit Learn Feature Extraction Text\n",
    "- Natural Language Tool Kit\n",
    "\n",
    "**Check:** Discussion: what are some real world applications of these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some additional notes on text packages\n",
    "\n",
    "If you are interested in working with text, eg on your capstone, you may find the following packages worth researching besides NLTK and Sklearn:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) is for topic modelling and implements Word2Vec, which can tell you distance between words (such as Queen to King vs wife to husband)\n",
    "- [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/) works with gensim to visualise topics in text\n",
    "- [Fuzzywuzzy](https://pypi.python.org/pypi/fuzzywuzzy) compares similarity between documents\n",
    "- [Textblob](https://textblob.readthedocs.io/en/dev/) wraps up some nltk functionality in an easy-to-use interface (eg it can do sentiment analysis in a few lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
