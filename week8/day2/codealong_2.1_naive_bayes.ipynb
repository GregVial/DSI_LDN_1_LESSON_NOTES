{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)  Naive Bayes Classifier\n",
    "Week 8 | Lesson 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe Naive Bayes\n",
    "- Choose a Naive Bayes implementation based on your use case\n",
    "- Implement a Naive Bayes model through scikit-learn\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Work with methods in scikit-learn\n",
    "- Conceptually explain the Bayesian posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A reminder of Bayes Theorem:\n",
    "\n",
    "\n",
    "### $$P\\left(\\;A\\;|\\;B\\;\\right) = \\frac{P\\left(\\;B\\;|\\;A\\;\\right)P\\left(\\;A\\;\\right)}{P\\;(\\;B\\;)}$$\n",
    "\n",
    "Or, somewhat more clearly for our uses:\n",
    "\n",
    "### $$P\\left(\\;hypothesis\\;|\\;data\\;\\right) = \\frac{P\\left(\\;data\\;|\\;hypothesis\\;\\right)P\\left(\\;hypothesis\\;\\right)}{P\\;(\\;data\\;)} $$\n",
    "\n",
    "\n",
    "- $P\\;(\\;hypothesis\\;)$ - the probability of the hypothesis before we see any data; this is the _prior_\n",
    "- $P\\;(\\;data\\;|\\;hypothesis\\;)$ - the probability of the data under the hypothesis; this is the _likelihood_\n",
    "- $P\\;(\\;hypothesis\\;|\\;data\\;)$ - the probability of the hypothesis after we see the data; this is the _posterior_\n",
    "- $P\\;(\\;data\\;)$ - the probability of the data under any hypothesis; this is the _normalising constant_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Bayes in supervised machine learning\n",
    "\n",
    "We can use this for classification problems; your hypothesis is hence the particular classification. Its canonical use case is text classification, where it works very well. The central difference for a Naive Bayes classification model is that it treats all predictors as conditionally independent. This makes its computation very fast, though of course not particularly realistic. For this reason it is often regarded as a base classifier against which others are judged for their ability to beat the Naive Bayes. What is surprising in the particular case of text classification is that Naive Bayes often actually outperforms models such as SVM whilst being computationally much faster too. This is hence the case where it is most often actually used in production, and in other scenarios it is more of interest in comparisons of multiple models as a baseline. \n",
    "\n",
    "Given X contains n boolean attributes, our formula would appear for some particular case as:\n",
    "\n",
    "$$P(\\;Y=y_i\\;|\\;X=x_k\\;) = \\frac{P(\\;X=x_k\\;|\\;Y=Y_i\\;)P(\\;Y=y_i\\;)}{\\Sigma_j\\;P(\\;X=x_k\\;|\\;Y=Y_i\\;)P(\\;Y=y_i\\;)}$$\n",
    "\n",
    "It should be stated that Naive Bayes is not the only possible Bayesian classifier; one could construct a Bayesian approach that did not rest on the independence assumption but this would be computationally intractable. Critically, given independence then our probability across all $X$ given $Y$ becomes a product for each $X_i$\n",
    "\n",
    " $$P(\\;X_1...X_n\\;|\\;Y\\;) = \\prod_i P(\\;X_i\\;|\\;Y\\;) $$\n",
    "\n",
    "Such that the fundamental equation for the Naive Bayes classifier becomes:\n",
    "### $$P(\\;Y=y_k\\;|\\;X_1...X_n\\;) = \\frac{P(\\;Y=y_k\\;) \\prod_i P(\\;X_i\\;|\\;Y=Y_k\\;)}{\\Sigma_j\\;P(\\;Y=y_j\\;) \\prod_i P(\\;X_i\\;|\\;Y=y_j\\;)}$$\n",
    "\n",
    "Given some new unseen instance $X\\;=\\;<X_1...X_n>$, this equation shows us how to calculate the probability that $Y$ will take on the given value $Y\\;=\\;y_k$. In particular, we can simplify even further to return the most probable value of $Y$:\n",
    "\n",
    "### $$Y \\leftarrow argmax\\; P(\\;Y=y_k\\;) \\prod_i P(\\;X_i\\;|\\;Y=Y_k\\;) $$\n",
    "\n",
    "We can estimate parameters with maximum likelihood by taking relative frequency counts for each case in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving toward a production implementation\n",
    "\n",
    "Possible issues to contend with:\n",
    "\n",
    "- [Underflow](http://stackoverflow.com/questions/3704570/in-python-small-floats-tending-to-zero). Probabilites may very very small, too small for floating point arithmetic. We can solve by leveraging:\n",
    "\n",
    "$$log(ab) = log\\ a + log\\ b$$\n",
    "\n",
    "$$exp(log\\ x) = x$$\n",
    "\n",
    "So $P_1\\ *\\ P_2\\ ...\\ *\\ P_2 = exp(log\\ P_1 + ... + log\\ P_n)$\n",
    "\n",
    "\n",
    "- '0' probabilities. What if you never saw a feature value in your training data? We can use smoothing:\n",
    "\n",
    "$$\\hat\\theta_i= \\frac{x_i + \\alpha}{N + \\alpha d}  \\qquad (i=1,\\ldots,d)$$\n",
    "\n",
    "- Continuous features. This brings us to *distributions*.\n",
    "\n",
    "### The likelihood functions\n",
    "\n",
    "Bayesians tend to talk in terms of distributions of belief. Rather than point estimates of probabilities, we can use distributions.\n",
    "\n",
    "- For a binary event, probability can be modeled with the **binomial distribution**.\n",
    "- For > 2 discrete outcomes, the **multinomial distribution**.\n",
    "- And if features are continuous? **Gaussian**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn the Naive Bayes classifier has those three variants (Binomial/Bernoulli, Multinomial, Normal/Gaussian).\n",
    "\n",
    "- [sklearn BernouilliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "- [sklearn MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [sklearn GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Naive Bayes Example: mobile customer conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_iOS_1</th>\n",
       "      <th>phone_iOS_0</th>\n",
       "      <th>location_uk_1</th>\n",
       "      <th>location_uk_0</th>\n",
       "      <th>login_facebook_1</th>\n",
       "      <th>login_facebook_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>purchase_premium</th>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>350</td>\n",
       "      <td>150</td>\n",
       "      <td>450</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_standard</th>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free_trial</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   phone_iOS_1  phone_iOS_0  location_uk_1  location_uk_0  \\\n",
       "purchase_premium           400          100            350            150   \n",
       "purchase_standard            0          300            150            150   \n",
       "free_trial                 100          100            150             50   \n",
       "\n",
       "                   login_facebook_1  login_facebook_0  \n",
       "purchase_premium                450                50  \n",
       "purchase_standard               300                 0  \n",
       "free_trial                       50               150  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple case Naive Bayes\n",
    "# Bernoulli case of variables (binary)\n",
    "\n",
    "frequency_table=pd.DataFrame(data=[[400,100,350,150,450,50], [0,300,150,150,300,0], [100,100,150,50,50,150]], \n",
    "                            columns=[\"phone_iOS_1\", \"phone_iOS_0\", \"location_uk_1\", \"location_uk_0\", \"login_facebook_1\", \"login_facebook_0\"],\n",
    "                            index=[[\"purchase_premium\",\"purchase_standard\",\"free_trial\"]])\n",
    "frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: because of the zeroes we can add 1 to every case to smooth it\n",
    "#frequency_table=frequency_table+1\n",
    "#frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.3\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# What are the priors?\n",
    "\n",
    "P_premium=np.sum(frequency_table.loc[\"purchase_premium\", [\"phone_iOS_1\", \"phone_iOS_0\"]])/(np.sum(frequency_table.loc[:, \"phone_iOS_1\"])+np.sum(frequency_table.loc[:, \"phone_iOS_0\"]))\n",
    "P_standard=np.sum(frequency_table.loc[\"purchase_standard\", [\"phone_iOS_1\", \"phone_iOS_0\"]])/(np.sum(frequency_table.loc[:, \"phone_iOS_1\"])+np.sum(frequency_table.loc[:, \"phone_iOS_0\"]))\n",
    "P_free=np.sum(frequency_table.loc[\"free_trial\", [\"phone_iOS_1\", \"phone_iOS_0\"]])/(np.sum(frequency_table.loc[:, \"phone_iOS_1\"])+np.sum(frequency_table.loc[:, \"phone_iOS_0\"]))\n",
    "print(P_premium)\n",
    "print(P_standard)\n",
    "print(P_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.7\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# What are the likelihoods?\n",
    "\n",
    "P_ios_given_premium=np.sum(frequency_table.loc[\"purchase_premium\", [\"phone_iOS_1\"]])/np.sum(frequency_table.loc[\"purchase_premium\", [\"phone_iOS_1\", \"phone_iOS_0\"]])\n",
    "P_uk_given_premium=np.sum(frequency_table.loc[\"purchase_premium\", [\"location_uk_1\"]])/np.sum(frequency_table.loc[\"purchase_premium\", [\"location_uk_1\", \"location_uk_0\"]])\n",
    "P_fb_given_premium=np.sum(frequency_table.loc[\"purchase_premium\", [\"login_facebook_1\"]])/np.sum(frequency_table.loc[\"purchase_premium\", [\"login_facebook_1\", \"login_facebook_0\"]])\n",
    "P_ios_given_standard=np.sum(frequency_table.loc[\"purchase_standard\", [\"phone_iOS_1\"]])/np.sum(frequency_table.loc[\"purchase_standard\", [\"phone_iOS_1\", \"phone_iOS_0\"]])\n",
    "P_uk_given_standard=np.sum(frequency_table.loc[\"purchase_standard\", [\"location_uk_1\"]])/np.sum(frequency_table.loc[\"purchase_standard\", [\"location_uk_1\", \"location_uk_0\"]])\n",
    "P_fb_given_standard=np.sum(frequency_table.loc[\"purchase_standard\", [\"login_facebook_1\"]])/np.sum(frequency_table.loc[\"purchase_standard\", [\"login_facebook_1\", \"login_facebook_0\"]])\n",
    "P_ios_given_free=np.sum(frequency_table.loc[\"free_trial\", [\"phone_iOS_1\"]])/np.sum(frequency_table.loc[\"free_trial\", [\"phone_iOS_1\", \"phone_iOS_0\"]])\n",
    "P_uk_given_free=np.sum(frequency_table.loc[\"free_trial\", [\"location_uk_1\"]])/np.sum(frequency_table.loc[\"free_trial\", [\"location_uk_1\", \"location_uk_0\"]])\n",
    "P_fb_given_free=np.sum(frequency_table.loc[\"free_trial\", [\"login_facebook_1\"]])/np.sum(frequency_table.loc[\"free_trial\", [\"login_facebook_1\", \"login_facebook_0\"]])\n",
    "\n",
    "print(P_ios_given_premium)\n",
    "print(P_uk_given_premium)\n",
    "print(P_fb_given_premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.65\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# What are the normalising constants? Also called the probability of the data\n",
    "# In general we do not need to calculate these\n",
    "# I just show you that in this particular case it is \n",
    "\n",
    "P_iOS=np.sum(frequency_table.loc[:, \"phone_iOS_1\"])/(np.sum(frequency_table.loc[:, \"phone_iOS_1\"])+np.sum(frequency_table.loc[:, \"phone_iOS_0\"]))\n",
    "P_uk=np.sum(frequency_table.loc[:, \"location_uk_1\"])/(np.sum(frequency_table.loc[:, \"location_uk_1\"])+np.sum(frequency_table.loc[:, \"location_uk_0\"]))\n",
    "P_fb=np.sum(frequency_table.loc[:, \"login_facebook_1\"])/(np.sum(frequency_table.loc[:, \"login_facebook_1\"])+np.sum(frequency_table.loc[:, \"login_facebook_0\"]))\n",
    "print(P_iOS)\n",
    "print(P_uk)\n",
    "print(P_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.252\n",
      "0.0\n",
      "0.01875\n"
     ]
    }
   ],
   "source": [
    "# So given the three predictors, which class is the customer most likely to sign up to?\n",
    "# Let's say the customer is 1 to all three\n",
    "\n",
    "P_premium_given_ios_uk_fb=P_ios_given_premium * P_uk_given_premium * P_fb_given_premium * P_premium\n",
    "P_standard_given_ios_uk_fb=P_ios_given_standard * P_uk_given_standard * P_fb_given_standard * P_standard\n",
    "P_free_given_ios_uk_fb=P_ios_given_free * P_uk_given_free * P_fb_given_free * P_free\n",
    "print(P_premium_given_ios_uk_fb)\n",
    "print(P_standard_given_ios_uk_fb)\n",
    "print(P_free_given_ios_uk_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930747922438\n",
      "0.0\n",
      "0.0692520775623\n"
     ]
    }
   ],
   "source": [
    "# We can normalise the probabilities by summing them to 1 since they are\n",
    "# mutually exclusive and collectively exhaustive\n",
    "# We could also use the normalising constants to do this\n",
    "\n",
    "P_premium_given_ios_uk_fb_normalised=P_premium_given_ios_uk_fb/(P_premium_given_ios_uk_fb+P_standard_given_ios_uk_fb+P_free_given_ios_uk_fb)\n",
    "P_standard_given_ios_uk_fb_normalised=P_standard_given_ios_uk_fb/(P_premium_given_ios_uk_fb+P_standard_given_ios_uk_fb+P_free_given_ios_uk_fb)\n",
    "P_free_given_ios_uk_fb_normalised=P_free_given_ios_uk_fb/(P_premium_given_ios_uk_fb+P_standard_given_ios_uk_fb+P_free_given_ios_uk_fb)\n",
    "print(P_premium_given_ios_uk_fb_normalised)\n",
    "print(P_standard_given_ios_uk_fb_normalised)\n",
    "print(P_free_given_ios_uk_fb_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the sklearn implementation, it expects an instance of every customer\n",
    "# rather than an aggregated summary such as the above, which is the easiest\n",
    "# to understand as the binary case. We will have to recreate our data not as a summary\n",
    "# but as a full table with every example case to be input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Naive Bayes Example: mobile customer conversion again - in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_iOS_1</th>\n",
       "      <th>phone_iOS_0</th>\n",
       "      <th>location_uk_1</th>\n",
       "      <th>location_uk_0</th>\n",
       "      <th>login_facebook_1</th>\n",
       "      <th>login_facebook_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>purchase_premium</th>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>350</td>\n",
       "      <td>150</td>\n",
       "      <td>450</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_standard</th>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free_trial</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   phone_iOS_1  phone_iOS_0  location_uk_1  location_uk_0  \\\n",
       "purchase_premium           400          100            350            150   \n",
       "purchase_standard            0          300            150            150   \n",
       "free_trial                 100          100            150             50   \n",
       "\n",
       "                   login_facebook_1  login_facebook_0  \n",
       "purchase_premium                450                50  \n",
       "purchase_standard               300                 0  \n",
       "free_trial                       50               150  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how I recreated the data, this results in a count of instances the same as the above \n",
    "# summary table gave us in a format that sklearn expects (each row being one customer)\n",
    "# y is the outcome with 1 = premium, 2 = standard, 3 = free\n",
    "\n",
    "X=pd.DataFrame(data={\"phone_ios\": np.ravel([[1]*400 + [0]*100 + [0]*300 + [1]*100 + [0]*100]), \n",
    "                     \"location_uk\": np.ravel([[1]*350 + [0]*150 + [1]*150 + [0]*150 + [1]*150 + [0]*50]),\n",
    "                     \"login_facebook\": np.ravel([[1]*450 + [0]*50 + [1]*300 + [1]*50 + [0]*150])})\n",
    "y=np.concatenate([np.ones(500), 2*np.ones(300), 3*np.ones(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on Bernoulli Naive Bayes, because all predictor values are binary\n",
    "from sklearn import naive_bayes\n",
    "nb=naive_bayes.BernoulliNB()\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted category for iOS = 1 , uk = 1 , fb = 1 :\n",
      "[ 1.]\n",
      "(note that 1 = premium, 2 = standard, 3 = free)\n",
      "With predicted probabilities (remember these are less reliable than other classifiers):\n",
      "[[ 0.93216829  0.00230087  0.06553084]]\n"
     ]
    }
   ],
   "source": [
    "# Going back to that one example we tested previously where all three predictors = 1\n",
    "\n",
    "print(\"predicted category for iOS = 1 , uk = 1 , fb = 1 :\")\n",
    "print(nb.predict(np.array([1,1,1]).reshape(1,-1)))\n",
    "print(\"(note that 1 = premium, 2 = standard, 3 = free)\")\n",
    "print(\"With predicted probabilities (remember these are less reliable than other classifiers):\")\n",
    "print(nb.predict_proba(np.array([1,1,1]).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86\n",
      "[[81  0 13]\n",
      " [ 8 60  0]\n",
      " [ 7  0 31]]\n"
     ]
    }
   ],
   "source": [
    "# General performance on test set for classification\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "predictions=nb.predict(X_test)\n",
    "print(accuracy_score(predictions, y_test))\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided practice: Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9408da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import data into a numpy array\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=Y, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "#Initialize a variable as the Guassian Naive Bayes classifier and fit it with the data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB()\n",
    "\n",
    "# Predict a few instances\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(clf.predict([[2, 1]]))\n",
    "\n",
    "# You could try plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive-Bayes classifier with SPAM (this data has already been processed)\n",
    "# and vectorized to numbered features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "spam_data = pd.read_csv('./assets/datasets/spam_base.csv', header=None)\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=spam_data.loc[:,0:53]\n",
    "y=spam_data.loc[:,57]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb=naive_bayes.MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions=nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883821932682\n",
      "[[443  19]\n",
      " [ 88 371]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(predictions, y_test))\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "How does Naive Bayes fit into your toolkit? What are the pros and cons? How do you choose between variants?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
