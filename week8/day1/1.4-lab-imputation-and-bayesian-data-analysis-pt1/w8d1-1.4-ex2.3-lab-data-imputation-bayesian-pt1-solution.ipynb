{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Missing Political Data - machine learning techniques for pre/post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This lab will focus on handling missing data in a new way: leveraging some of the models you've learned to use.\n",
    "\n",
    "In general this topic is more on the \"art\" side of the science/art spectrum, but there are some well-established ways to deal with and impute missing data, depending on what you want to accomplish in the end (increase the power, remove NaNs, impute with a numerical/label to prevent errors from your ML algorithms, etc.). \n",
    "\t\n",
    "Our overall goal is to see that there can be a \"functional relationship\" between the \"missingness\" of the data, and features found in our data. By doing this, we can categorize the kind of \"missingness\" we are dealing with for a particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Types of \"Missingness\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Types of \"Missingness\"\n",
    "| Type  | Description  | \n",
    "|---|---|\n",
    " | Missing Completely at Random  | This is basically the best scenario, all NaN, NA, or blanks are distributed totally at random can be safely omitted  |\n",
    " | Missing at Random  | This is less strong, but is \"random\" given the sample you are using. This is what we're aiming at for our analysis; functionally, we want to show that our missing data isn't dependent on data we haven't observed or accounted for in our dataset   | \n",
    " | Missing not at Random  | \"There is a data generating process that yields missing values\". Basically, it means there is some \"pattern\" to the 'missingness' |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Inclusion Indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As stated, the type of “missingness” we are most concerned about is the last row, \"Missing not at Random\". If there is a data generating process, this means we can model the “missingness” in our data set. If we can convincingly show that this model accounts for \"most\" (we're not being stringent statisticians, so that word will be left up to you to define) of the observable variation, we can be (relatively) well-at-ease that our \"missingness\" isn't functionally related to some features we don't have control/accounted/recorded in our data.\n",
    "\n",
    "Before we move forward, we have to define the \"inclusion indicator\". We say I is an inclusion indicator if : $$\\begin{array}{cc}\n",
    "  I=\\{ & \n",
    "    \\begin{array}{cc}\n",
    "      1 & x: missing \\\\\n",
    "      0 & x: \\neg{missing} \\\\\n",
    "    \\end{array}\n",
    "\\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up data with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load up polling data. However we will take the analysis much broader this time, and we will be using a version of the data set where we have not removed missing values... because after all, that's the point of this entire lab! \n",
    "\n",
    "So load up the data and the libraries we will need to get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import math\n",
    "import pylab as py\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bush</th>\n",
       "      <th>state</th>\n",
       "      <th>edu</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bush  state  edu  age\n",
       "0   1.0      7    2    2\n",
       "1   1.0     33    4    3\n",
       "2   0.0     20    2    1\n",
       "3   1.0     31    3    2\n",
       "4   1.0     18    3    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_poll = pd.read_csv('./assets/datasets/polls_new.csv')\n",
    "del pre_poll['Unnamed: 0']\n",
    "pre_poll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Construct the Inclusion indicator and append the column to the table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an 'inclusion' indicator column that will be 1 when bush is missing a value, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_poll['inclusion'] = np.where(pd.isnull(pre_poll['bush']), 1, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Prepare your data by converting it into numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our ML work will be better suited if the input data is contained in a numpy object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bush</th>\n",
       "      <th>state</th>\n",
       "      <th>edu</th>\n",
       "      <th>age</th>\n",
       "      <th>inclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bush  state  edu  age  inclusion\n",
       "0   1.0      5    1    1          0\n",
       "1   1.0     30    3    2          0\n",
       "2   0.0     17    1    0          0\n",
       "3   1.0     28    2    1          0\n",
       "4   1.0     15    2    0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encode = preprocessing.LabelEncoder()\n",
    "pre_poll['age'] = encode.fit_transform(pre_poll.age) \n",
    "pre_poll['state'] = encode.fit_transform(pre_poll.state)\n",
    "pre_poll['edu'] = encode.fit_transform(pre_poll.edu)\n",
    "\n",
    "\n",
    "pre_poll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3 - Split the data  70/30 train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in the ordinary way, making sure you have a 70/30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_poll['train'] = np.random.uniform(0, 1, len(pre_poll)) <= .70\n",
    "pre_poll_train = pre_poll[pre_poll['train'] == True]\n",
    "test = pre_poll[pre_poll['train'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait... how can we tell if something is \"Missing not at random\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good question. One way is to understand \"how much\" of the variation in your data your model is accounting for. We'll do some preliminary work on that front here, but I'm going to ask you to ask yourself:\n",
    "\n",
    "1. How can I apply what I've learned in regressions to this problem? \n",
    "2. What are other metrics I could use to account for variation in data outside of regressions? \n",
    "\n",
    "One approach we've strongly pointed towards is to construct regression models with the inclusion indicator as a target, and see what sort of performance you can get out of those family of techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression to model the \"missingness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4 - Build a classical logistic regression to model the inclusion indicator as a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.420793\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>inclusion</td>    <th>  No. Observations:  </th>  <td>  9478</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  9475</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 28 Nov 2016</td> <th>  Pseudo R-squ.:     </th> <td>-0.001427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>15:10:57</td>     <th>  Log-Likelihood:    </th> <td> -3988.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -3982.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 1.000</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>edu</th>   <td>   -0.6257</td> <td>    0.024</td> <td>  -25.919</td> <td> 0.000</td> <td>   -0.673    -0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state</th> <td>   -0.0174</td> <td>    0.002</td> <td>  -10.555</td> <td> 0.000</td> <td>   -0.021    -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>   <td>   -0.2479</td> <td>    0.024</td> <td>  -10.441</td> <td> 0.000</td> <td>   -0.294    -0.201</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              inclusion   No. Observations:                 9478\n",
       "Model:                          Logit   Df Residuals:                     9475\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Mon, 28 Nov 2016   Pseudo R-squ.:               -0.001427\n",
       "Time:                        15:10:57   Log-Likelihood:                -3988.3\n",
       "converged:                       True   LL-Null:                       -3982.6\n",
       "                                        LLR p-value:                     1.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "edu           -0.6257      0.024    -25.919      0.000        -0.673    -0.578\n",
       "state         -0.0174      0.002    -10.555      0.000        -0.021    -0.014\n",
       "age           -0.2479      0.024    -10.441      0.000        -0.294    -0.201\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is my favorite logistic implementation, very simple, close to R's lm. \n",
    "import statsmodels.formula.api as sm\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "rhs_columns = ['edu', 'state', 'age']\n",
    "inclusion_lm = sm.Logit(pre_poll_train['inclusion'], pre_poll_train[rhs_columns]).fit()\n",
    "\n",
    "inclusion_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5 - Build a vector of prediction from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09907105  0.14831913  0.16199408 ...,  0.17487003  0.21356684\n",
      "  0.14191949]\n"
     ]
    }
   ],
   "source": [
    "y_pred = inclusion_lm.predict(test[rhs_columns])\n",
    "print y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using K-Nearest Neighbor for imputing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 6 - Build a K-NN model (k = 5), to model the inclusion indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this model isn't really to shed more light on the \"missingness\", but rather to actually impute values into our column of data that contains missing values. Still, it's a good exercise to go through. After you've done the imputation, take a random subset of these imputed values and think about the results, is doing this a good way to fill in values? Would it be easier to do something simpler i.e. take the average for numerical data, or just select some label as fill-in for categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_impute = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_impute.fit(pre_poll_train[rhs_columns], pre_poll_train['inclusion'])\n",
    "\n",
    "knn_pred = knn_impute.predict(test[rhs_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7 - Build a Random forest to model the inclusion indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the KNN, this is more about actually doing the imputation. However still a good review/exercise, compare your results with the KNN. How can we objectively measure relative performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "random_for = RandomForestClassifier(n_estimators=1000)\n",
    "random_for.fit(pre_poll_train[rhs_columns], pre_poll_train['inclusion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing some basic comparisons of results and forecasting efficaciousness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare our results -- construct ROC scores for each of the 3 methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for your non-Bayesian Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.615688\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test['inclusion'], y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.648414\n"
     ]
    }
   ],
   "source": [
    "random_pred = random_for.predict_proba(test[rhs_columns]); random_pred\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test['inclusion'], random_pred[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.525648\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test['inclusion'], knn_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open Ended Questions** Can we be fairly confident that there is some kind of functional relationship between the indicator variable and the few columns we studied in our data set? Is it obvious that there are probably other factors impacting \"missingness\" from this data? Which type of \"missingness\" are we probably in, and what does that say about the state of our missing data and how we should approach modeling on this data set in the future? What further actions can we take to augment this analysis?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
