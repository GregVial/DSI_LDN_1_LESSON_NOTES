{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Methods in sklearn Lab\n",
    "\n",
    "In this lab we will explore feature selection on the Titanic Dataset. First of all let's load a few things:\n",
    "\n",
    "- The training set from lab 2.3\n",
    "- The union we have saved in lab 2.3\n",
    "\n",
    "\n",
    "You can load the titanic data as follows:\n",
    "\n",
    "    psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic\n",
    "    password: gastudents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic')\n",
    "\n",
    "# We will pull in the Titanic data as before\n",
    "df = pd.read_sql('SELECT * FROM train', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5924806 ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "        1.        , -0.50244517])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import dill\n",
    "\n",
    "# I've set up a preset union object here, from lab 2.3. You can complete the lab with this but you may find it more\n",
    "# satisfying to load the version that you did in that lab (and hopefully saved as union.dill.gz)\n",
    "with gzip.open('union_preset.dill.gz') as fin:\n",
    "    union = dill.load(fin)\n",
    "    \n",
    "X = df[[u'Pclass', u'Sex', u'Age', u'SibSp', u'Parch', u'Fare', u'Embarked']]\n",
    "y = df[u'Survived']\n",
    "\n",
    "X_transf = union.fit_transform(X)\n",
    "X_transf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 Column names\n",
    "\n",
    "The column names have become lost along the way. We can manually add them back in:\n",
    "\n",
    "`union = make_union(age_pipe, one_hot_pipe, gender_pipe, fare_pipe)`\n",
    "\n",
    "- age_pipe => 'scaled_age'\n",
    "- one_hot_pipe => 'Pclass_2', 'Pclass_3', 'Embarked_Q', 'Embarked_S'\n",
    "- gender_pipe => 'male'\n",
    "- fare_pipe => 'scaled_fare'\n",
    "\n",
    "We need to:\n",
    "\n",
    "1. Create a new pandas dataframe `Xt` with the appropriate column names and fill it with the `X_transf` data.\n",
    "2. Notice that we discard the columns: u'SibSp', u'Parch', that is unless you covered this part in lab 2.3 (this was an optional bonus). So we ignore them in this analysis, unless you dealt with them there in which case do keep using them.\n",
    "3. The order of columns should match the order you place the pipelines in the union; I have added the code I used above from lab 2.3 which was used to make the union_preset.dill.gz file. If you are importing your own union object you may wish to verify the code you used by reopening the notebook from lab 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_age</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>male</th>\n",
       "      <th>scaled_fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.592481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.502445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.638789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.284663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.488854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.486337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_age  Pclass_2  Pclass_3  Embarked_Q  Embarked_S  male  scaled_fare\n",
       "0   -0.592481       0.0       1.0         0.0         1.0   1.0    -0.502445\n",
       "1    0.638789       0.0       0.0         0.0         0.0   0.0     0.786845\n",
       "2   -0.284663       0.0       1.0         0.0         1.0   0.0    -0.488854\n",
       "3    0.407926       0.0       0.0         0.0         1.0   0.0     0.420730\n",
       "4    0.407926       0.0       1.0         0.0         1.0   1.0    -0.486337"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col_names = ['scaled_age', 'Pclass_2', 'Pclass_3', 'Embarked_Q', 'Embarked_S','male', 'scaled_fare']\n",
    "Xt = pd.DataFrame(X_transf, columns=new_col_names)\n",
    "Xt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "\n",
    "Let's use the `SelectKBest` method in scikit learn to see which are the top 5 features (out of how many?). We will use the f test for classification method that we discussed in the lecture (this is actually the default but we import it just to be explicit about our methodology, this is particularly important for this approach because it can still give\n",
    "results if you apply the test for a countinous output rather than a classification output - but this would be meaningless, hence always have a think about which test you are actually applying with these methods).\n",
    "\n",
    "- What are the top 5 features for `Xt`?\n",
    "- Check the methods associated with SelectKBest to see how to interact with it and return the columns of interest (eg get_support returns booleans you can use for indexing the dataframe)\n",
    "\n",
    "=> store them in a variable called `kbest_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>male</th>\n",
       "      <th>scaled_fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.502445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.488854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.486337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_2  Pclass_3  Embarked_S  male  scaled_fare\n",
       "0       0.0       1.0         1.0   1.0    -0.502445\n",
       "1       0.0       0.0         0.0   0.0     0.786845\n",
       "2       0.0       1.0         1.0   0.0    -0.488854\n",
       "3       0.0       0.0         1.0   0.0     0.420730\n",
       "4       0.0       1.0         1.0   1.0    -0.486337"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selected_data = selector.fit_transform(Xt, y)\n",
    "kbest_columns = Xt.columns[selector.get_support()]\n",
    "Xtbest = pd.DataFrame(selected_data, columns=kbest_columns)\n",
    "Xtbest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.72170837e-02   5.29365528e-03   5.51028100e-23   9.13353235e-01\n",
      "   3.03611106e-06   1.40606613e-69   6.12018934e-15]\n",
      "Index([u'scaled_age', u'Pclass_2', u'Pclass_3', u'Embarked_Q', u'Embarked_S',\n",
      "       u'male', u'scaled_fare'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# If you manage the above, you might be a bit unclear about how it has selected these\n",
    "# top five. If you call selector.pvalues_ you can return the p value results of the\n",
    "# comparisons of the inputs to see which are the more significant\n",
    "print(selector.pvalues_)\n",
    "print(Xt.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursive Feature Elimination\n",
    "\n",
    "`Scikit Learn` also offers recursive feature elimination as a class named `RFE`. Use it in combination with a logistic regression model to see what features would be kept with this method.\n",
    "\n",
    "=> store them in a variable called `rfecv_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'scaled_age', u'Pclass_2', u'Pclass_3', u'Embarked_S', u'male'], dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, step=1, n_features_to_select=5)\n",
    "selector = selector.fit(Xt, y)\n",
    "rfecv_columns = Xt.columns[selector.support_]\n",
    "rfecv_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 3, 1, 1, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus - also try from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression coefficients\n",
    "\n",
    "Let's see what we get with the Logistic Regression coefficients.\n",
    "\n",
    "- Create a logistic regression model\n",
    "- Perform grid search over penalty type and C strength in order to find the best parameters\n",
    "- Sort the logistic regression coefficients by absolute value. Do the top 5 correspond to those above? They may not because these selection methods are distinct. So it's up to you to decide on the best approach.\n",
    "\n",
    "=> choose which ones you would keep and store them in a variable called `lr_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                                            'penalty': ['l1', 'l2']})\n",
    "model.fit(Xt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79573512906846244"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surv coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>2.563142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass_3</th>\n",
       "      <td>2.258976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass_2</th>\n",
       "      <td>0.935721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>0.552196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_age</th>\n",
       "      <td>0.429123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_fare</th>\n",
       "      <td>0.009531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Surv coeff\n",
       "male           2.563142\n",
       "Pclass_3       2.258976\n",
       "Pclass_2       0.935721\n",
       "Embarked_S     0.552196\n",
       "scaled_age     0.429123\n",
       "scaled_fare    0.009531\n",
       "Embarked_Q     0.000000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame(model.best_estimator_.coef_, columns = Xt.columns)\n",
    "coeffs_t = coeffs.transpose()\n",
    "coeffs_t.columns = ['Surv coeff']\n",
    "coeffs_t_abs=coeffs_t.abs().sort_values('Surv coeff', ascending=False)\n",
    "coeffs_t_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the top 5 features\n",
    "lr_columns=coeffs_t_abs.iloc[0:5].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare features sets\n",
    "\n",
    "Use the `best estimator` from question 4 on the 3 different feature sets:\n",
    "\n",
    "- `kbest_columns`\n",
    "- `rfecv_columns`\n",
    "- `lr_columns`\n",
    "- `all_columns`\n",
    "\n",
    "Questions:\n",
    "\n",
    "- Which scores the highest? (use cross_val_score)\n",
    "- Is the difference significant?\n",
    "- discuss in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean score</th>\n",
       "      <th>std score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kbest</th>\n",
       "      <td>0.768799</td>\n",
       "      <td>0.005723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfecv</th>\n",
       "      <td>0.796857</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.769921</td>\n",
       "      <td>0.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.796857</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean score  std score\n",
       "kbest    0.768799   0.005723\n",
       "rfecv    0.796857   0.001587\n",
       "lr       0.769921   0.009655\n",
       "all      0.796857   0.001587"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "def score(X):\n",
    "    scores = cross_val_score(model.best_estimator_, X, y)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "all_scores = [\n",
    "    score(Xt[kbest_columns]),\n",
    "    score(Xt[rfecv_columns]),\n",
    "    score(Xt[lr_columns]),\n",
    "    score(Xt)]\n",
    "\n",
    "pd.DataFrame(all_scores, columns=['mean score', 'std score'], index = ['kbest', 'rfecv', 'lr', 'all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Use a bar chart to display the logistic regression coefficients. Start from the most negative on the left. Can you come up with bar chart rankings for the other methods also? How about a heat map like we talked about in the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117fcb790>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAGvCAYAAABVf9xJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIaxvCXJKlhDH9JkhrG8JckqWH+P5Vv2qBF77l9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11887fd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeffs_t.sort_values('Surv coeff').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
