{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Selection\n",
    "Week 5 | Lesson 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- describe what feature selection means and why it is important\n",
    "- evaluate several techniques for feature selection\n",
    "- use L1 and L2 regularization for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- perform regression\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening\n",
    "In this class we will go over feature selection.\n",
    "Feature selection is an important part of the model building process and there are several ways to approach it.\n",
    "\n",
    "**Check:** Can anyone guess why we may want to select features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: Feature Selection\n",
    "\n",
    "In machine learning we usually start from a vector of features that encapsulate different aspects of our data and we use these to train a model that predicts a target variable.\n",
    "Many of the datasets we have encountered so far were in tabular format, and features were well defined already. This is often not the case at all!\n",
    "\n",
    "**Check** Can you give a couple of examples of data where features must be extracted?\n",
    "\n",
    "\n",
    "For example, in the case of text data, a data-point usually corresponds to a document or a sentence, and we have to use feature extraction techniques to map each document to a vector of numbers. Similarly, to classify images, we must first represent them as a vector of numbers.\n",
    "As we have seen with the `CountVectorizer` for text data, feature extraction can result in a very large number of features, some of which may have more predictive power than others.\n",
    "\n",
    "|Text|the|cat|is|on|table|blue|sky|...|\n",
    "|----|\n",
    "|The cat is on the table|2|1|1|1|1|0|0|...|\n",
    "|The table is blue|1|0|1|0|1|1|0|...|\n",
    "|The sky is blue|1|0|1|0|0|0|1|...|\n",
    "|...|...|...|...|...|...|...|...|...|\n",
    "\n",
    "In the above example the word `the` probably has zero predictive power.\n",
    "\n",
    "In a different scenario, the tabular data may be a result of a database dump where irrelevant columns were not removed.\n",
    "\n",
    "\n",
    "|CustomerID |CompanyName |ContactName | ContactTitle |Address|City | Region | PostalCode | Country |Phone | Fax|\n",
    "|---|\n",
    "|ALFKI| Alfreds Futterkiste| Maria Anders | Sales Representative | Obere Str. 57 | Berlin|| 12209| Germany | 030-0074321| 030-0076545|\n",
    "|ANATR| Ana Trujillo Emparedados y helados | Ana Trujillo | Owner| Avda. de la Constitución 2222 | México D.F. || 05021| Mexico| (5) 555-4729 | (5) 555-3745|\n",
    "|ANTON| Antonio Moreno Taquería| Antonio Moreno | Owner| Mataderos2312 | México D.F. || 05023| Mexico| (5) 555-3932 ||\n",
    "|...|...|...|...|...|...|...|...|...|...|...|\n",
    "\n",
    "Feature Selection is a way to reduce the number of features to simplify the model while retaining its predictive power.\n",
    "\n",
    "\n",
    "\n",
    "### Bottom up feature selection / forward stepwise selection\n",
    "\n",
    "One way to select features is to start with no predictors, so for example a logistic regression against a constant as the sole feature. We then add in each of the p possible features and find the single feature that gives the highest score metric we are testing, and then iteratively add the other features one by one, each time checking how much the score improves and adding specifically the one feature that improves the score the most each time. Score improvement will hence be smaller and smaller the more features we add; adding complexity to the model will yield diminishing returns.\n",
    "\n",
    "One can then set a criteria to only retain the first N features or the first features that achieve e.g. 90% as good a score as the model with all p features (bear in mind this could be _a lot_ fewer features than p).\n",
    "\n",
    "You might notice that we do not test every possible model this way, what about all the other combinations of features that are possible? This process is computationally more feasible than an approach taking all possible combinations and likely to yield similar results. Fitting all possible feature combinations is known as the **best subset selection**.\n",
    "\n",
    "For 20 possible features, there are 211 models to be tested in forward stepwise selection. For best subset selection, there are 1,048,576 models to be tested. How long do you have? \n",
    "\n",
    "\n",
    "### Top Down feature selection / backward stepwise selection, and hybrid approaches\n",
    "\n",
    "Obviously, we can do this process in reverse and start with the full model with all features and remove the least predictive one at each step. This would not necessarily end with the same features as the forward stepwise approach. More interestingly, a hybrid approach could be taken with, e.g. a forward stepwise approach that then performans a backward analysis to check if a different feature might be removed each time a new one has been added. This more closely approximates a full best subset fit whilst still involving a smaller number of total model fits.\n",
    "\n",
    "\n",
    "### Regularisation\n",
    "\n",
    "Regularisation imposes a global constraint on the values of the parameters that define the model. The regularized model is found solving a new minimisation problem where two terms are present: the term defining the model and the term defining the regularisation.\n",
    "\n",
    "In general, a regularisation term $R(f)$ is introduced to a general loss function:\n",
    "\n",
    "$$\\min_f \\sum_{i=1}^{n} V(f(\\hat x_i), \\hat y_i) + \\lambda R(f)$$\n",
    "\n",
    "for a loss function $V$ that describes the cost of predicting $f(x)$ when the label is $y$, such as the square loss or hinge loss, and for the term $\\lambda$ which controls the importance of the regularization term. $R(f)$ is typically a penalty on the complexity of $f$, such as restrictions for smoothness or bounds on the vector space norm.\n",
    "\n",
    "#### l1 and l2 regularisation\n",
    "\n",
    "To reiterate, for the case where our model $f$ is parametric (e.g. like in linear models), we can express the regularisation term as a norm on the parameters. Depending on the kind of norm chosen for the parameters regularisation can be:\n",
    "\n",
    "l1 or Lasso regularisation:\n",
    "$$ R(f) \\propto \\sum{|\\beta_i|}$$\n",
    "\n",
    "or\n",
    "\n",
    "l2 or Ridge regularisation:\n",
    "$$ R(f) \\propto \\sum{\\beta_i^2}$$\n",
    "\n",
    "where $\\beta_i$ are the parameters of the model. Critically, l1 regularisation can set parameter coefficients to zero and thus act as a feature selection method.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Some models offer an intrinsic way of evaluating feature importance. For example, as we will see next week, _Decision Trees_ have a way to assess how much each feature is contributing to the overall prediction.\n",
    "\n",
    "Logistic Regression also offers an intrinsic way of assessing feature importance. In fact, since the Logistic Regression model is linear in the features, the size of each coefficient represents the impact a unit change in that feature has on the overall model. In order to interpret this as feature importance, we need to make sure that features are normalized to have the same scale, or a unit change (i.e. the coefficient) would imply varying change in the odds outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Feature importance in Logistic Regression\n",
    "\n",
    "As we have said, the coefficients in logistic regression can be interpreted as feature importance if the data is normalised. We will show this on the Iris dataset for convenience.\n",
    "\n",
    "First load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's initialize a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` class exposes an attribute called `coef_`. Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41498833,  1.46129739, -2.26214118, -1.0290951 ],\n",
       "       [ 0.41663969, -1.60083319,  0.57765763, -1.38553843],\n",
       "       [-1.70752515, -1.53426834,  2.47097168,  2.55538211]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! According to the documentation this is:\n",
    "\n",
    "    coef_ : array, shape (n_classes, n_features)\n",
    "        Coefficient of the features in the decision function.\n",
    "        \n",
    "Let's display it in a nicer way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>0.414988</td>\n",
       "      <td>1.461297</td>\n",
       "      <td>-2.262141</td>\n",
       "      <td>-1.029095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.416640</td>\n",
       "      <td>-1.600833</td>\n",
       "      <td>0.577658</td>\n",
       "      <td>-1.385538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>-1.707525</td>\n",
       "      <td>-1.534268</td>\n",
       "      <td>2.470972</td>\n",
       "      <td>2.555382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa               0.414988          1.461297          -2.262141   \n",
       "versicolor           0.416640         -1.600833           0.577658   \n",
       "virginica           -1.707525         -1.534268           2.470972   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.029095  \n",
       "versicolor         -1.385538  \n",
       "virginica           2.555382  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Can we conclude that `petal length (cm)` is the most significant feature to identify `setosa` ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the data and repeat the exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>-0.810166</td>\n",
       "      <td>1.393699</td>\n",
       "      <td>-1.687386</td>\n",
       "      <td>-1.518991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.130380</td>\n",
       "      <td>-1.246338</td>\n",
       "      <td>0.789195</td>\n",
       "      <td>-0.889440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.144535</td>\n",
       "      <td>1.863173</td>\n",
       "      <td>2.698873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa              -0.810166          1.393699          -1.687386   \n",
       "versicolor           0.130380         -1.246338           0.789195   \n",
       "virginica            0.012990         -0.144535           1.863173   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.518991  \n",
       "versicolor         -0.889440  \n",
       "virginica           2.698873  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_norm =  StandardScaler().fit_transform(X)\n",
    "\n",
    "model.fit(X_norm, y)\n",
    "\n",
    "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that normalization did change the sign and magnitude of the LR coefficients. Also notice that the Logistic Regression class has a `penalty` parameter that allows us to choose between `l1` and `l2` regularization. Notice that some of the solvers only support `l2` regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Try changing the penalty to `l1`, do the coefficients change?\n",
    "\n",
    "**Optional Check** Check score with `cross_val_score` and select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.198029</td>\n",
       "      <td>-4.427947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.029515</td>\n",
       "      <td>-1.239732</td>\n",
       "      <td>0.762604</td>\n",
       "      <td>-0.769475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.480737</td>\n",
       "      <td>2.716761</td>\n",
       "      <td>4.580145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa               0.000000          1.198029          -4.427947   \n",
       "versicolor           0.029515         -1.239732           0.762604   \n",
       "virginica            0.000000         -0.480737           2.716761   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa              0.000000  \n",
       "versicolor         -0.769475  \n",
       "virginica           4.580145  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression(penalty='l1')\n",
    "model2.fit(X_norm, y)\n",
    "coeffs = pd.DataFrame(model2.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ridge: ', 0.90666666666666662)\n",
      "('Lasso: ', 0.93333333333333324)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "accuracies = cross_val_score(model, X_norm, y, cv=5)\n",
    "print(\"Ridge: \",np.mean(accuracies))\n",
    "accuracies2 = cross_val_score(model2, X_norm, y, cv=5)\n",
    "print(\"Lasso: \",np.mean(accuracies2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Visualizing feature importance\n",
    "\n",
    "In some cases we may want to visualize feature importance in a plot. One way to do this is to display the matrix of coefficients with a color scale. We can do this using matplotlib as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAErCAYAAAAyrlO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5P8BRxxEemoGCZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb603128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first import matplolib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# then create a figure and a plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# display the matrix\n",
    "cax = ax.matshow(coeffs)\n",
    "\n",
    "# add colorbars and tics\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(coeffs)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(coeffs.columns), rotation=45)\n",
    "ax.set_yticklabels(['']+list(coeffs.index))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Topic\n",
    "Scikit learn offers several other regularization methods. The following exercise is in threes:\n",
    "\n",
    "Go to the [documentation page](http://scikit-learn.org/stable/modules/feature_selection.html), pick one method and read about it for 10 minutes.\n",
    "\n",
    "Try to answer the following questions:\n",
    "\n",
    "1. is it top-down or bottom-up?\n",
    "- does it work for both regression and classification?\n",
    "- can it be used in a pipeline?\n",
    "\n",
    "In the last 10 minutes expose your findings to the class\n",
    "\n",
    "1. feature_selection.GenericUnivariateSelect([...])\n",
    "- feature_selection.SelectPercentile([...])\n",
    "- feature_selection.SelectKBest([score_func, k])\n",
    "- feature_selection.SelectFpr([score_func, alpha])\n",
    "- feature_selection.SelectFdr([score_func, alpha])\n",
    "- feature_selection.SelectFromModel(estimator)\n",
    "- feature_selection.SelectFwe([score_func, alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination\n",
    "\n",
    "I will explicitly discuss recursive feature elimination (RFE) in sklearn since we use this in the upcoming lab and it might be a bit unclear. This involves fitting models several times. You fit the model once with all the features, then you drop one (or more, you can set this parameter) and then refit, then drop one and refit (until you get to the number of features you input also as a parameter at the start). How do you decide which of the features you want to drop? In the case of logistic regression it is based on the coefficient size with some extra corrections. Hence this is a version of top down (or backwards) stepwise selection, with attempts to be more intelligent than only comparing coefficient size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Testing for Feature Selection\n",
    "\n",
    "Some of the methods we will see in the lab use some statistical tests to determine if features are significant. We have talked about statistical testing in the context of t-tests. In that case, we were looking to see if\n",
    "two statistics (such as a mean value for the heights of a population) were significantly different from each other by assuming either a normal (for large numbers of values, where we can approximate a normal distribution) or a t (for smaller numbers of values i.e. below 30) distribution. For the normal case this is called a z-test, for the t a t-test. The distinction in the normal and t distributions was in the kurtosis, such that a t distribution would have a greater tolerance for more extreme values and judge these to be less significant than a normal distribution.\n",
    "\n",
    "Another method used to compare two or more statistics (again such as means of heights of different subgroups of a population) at the same time is the analysis of variance (ANOVA). That's useful because the t test just compares two values. In this case an F-statistic is calculated (please note this is totally unrelated to the F1 score you calculate as a metric for classification problems) and an F-test\n",
    "is performed on the F-distribution to see if the F statistic is significant, again returning a p-value.\n",
    "\n",
    "Hence the \n",
    "flow of such tests follow the same format. You calculate a statistic (Z-statistic, t-stastic, F-statistic), compare the value to the appropriate distribution (Z distribution, which is just the normal distribution, the t distribution, or the F distribution) and return a p value giving the probability of the statistic occuring. So the difference is in the calculation of the statistic and the exact form of the distribution used for the testing. The F distribution is positively skewed, unlike the normal and t distributions which are symmetric.\n",
    "\n",
    "For the F-statistic, we calculate a sum of the variances of each grouping compared with the overall mean of the total, and divide this by a sum of the variances of each data point within the groupings compared with the mean of that grouping (with a correction for number of terms). That is to say, _a term for the variability between groups divided by a term for the variability within groups_.\n",
    "\n",
    "\n",
    "In a classification problem this can be applied by considering the groupings to be each classification outcome. So to perform the test we need to input both our feature matrix and our outputs. Then the features are subset into those corresponding to each of the possible outcomes and we see if the means of the features in each grouping are significantly different from each other. If they are not then the feature seems to not provide much predictive power. \n",
    "\n",
    "The equation below shows the F-statistic calculation for a binary outcome classification where (+) refers to the grouping of one outcome and (-) the grouping for the second outcome. Note that this generalises to multivariate classification, which is why it is convenient to use it even for the binary case rather than another test.\n",
    "\n",
    "![](./assets/images/f_statistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "Today we have learned about some ways to select features by importance. This will be particularly useful when we deal with a large number of features. We have already talked about regularisation strength in the context of a grid search as a more indirect method of feature selection. Now we will talk about a process of preselecting features using sklearn, in lab 4.2, in which we might opt to explicitly drop some features prior to the model fit."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
