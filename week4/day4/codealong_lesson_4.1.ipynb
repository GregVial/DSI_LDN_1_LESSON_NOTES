{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Model Evaluation, Multinomial Logistic Regression, and Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge (l2) and Lasso (l1) Regularisation applied to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge and lasso regularisation have been discussed in the context of linear regression. Similarly in the case of logistic regression, ridge is adding a penalty term in relation to the square of coefficients (hence l2) and lasso is adding a term in relation to the absolute value of coefficients (l1) when penalising increasing magnitude of coefficients in the fit. \n",
    "\n",
    "This aims to reduce the influence of individual terms in order to mitigate overfitting (this forms a prior on our expected outcome, in the context of Bayesian statistics). This penalty term is then removed from the log likelihood calculation (which is the term we seek to maximise when iterating the logistic regression function to find the fit of coefficients, this is something that is performed under the hood of the fitting algorithm).\n",
    "\n",
    "Something we did not discuss is that in the sklearn implementation of logistic regression, such regularisation is applied by default. In fact l2 regularisation is the default, and it is not actually possible to turn this off (only to select l2 or l1). How can we affect this regularisation, to tune its impact? We have a C score hyperparameter which we can tune, which is the inverse of the regularisation strength (usually termed lambda). Slightly confusingly, this means that the larger C is the smaller the regularisation effect. If we wanted to produce a case without regularisation, we can hence set C to an absurdly high number (such as 1e10).\n",
    "\n",
    "But how can we decide on what the best value of C is for our particular case? We can perform a grid search to test a range of values, and sklearn implements this quite straightforwardly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and setting C hyperparameter in the context of Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to cover too many topics at once, let's check out the grid search process for a binomial logistic regression\n",
    "before we take a look at the multinomial case. We can go back to a dataset we modelled previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "      <th>is_red</th>\n",
       "      <th>high_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality color  is_red  high_quality  \n",
       "0      9.4        5   red     1.0           0.0  \n",
       "1      9.8        5   red     1.0           0.0  \n",
       "2      9.8        5   red     1.0           0.0  \n",
       "3      9.8        6   red     1.0           0.0  \n",
       "4      9.4        5   red     1.0           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the wine dataset - we are predicting high quality\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/demo-datasets/wine.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's fit a model with sklearn\n",
    "X=df[[\"density\",\"pH\", \"alcohol\"]]\n",
    "y=df[\"high_quality\"]\n",
    "\n",
    "# train test split with a stratify term on the outcome to keep datasets balanced on the three outcomes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1=LogisticRegression()\n",
    "model1.fit(X_train, Y_train)\n",
    "predictions1=model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1666,   57],\n",
       "       [ 341,   81]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix=confusion_matrix(Y_test, predictions1)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we know, this output of a confusion matrix is a bit awkward and confusing\n",
    "# Here is some nice code to plot a confusion matrix in an easier to read way,\n",
    "# this is from sklearn's documentation\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=model1.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1666,   57],\n",
       "       [ 341,   81]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a great classifer, but anyway let's talk about the regularisation going on here\n",
    "# in fact, LogisticRegression() is implementing the following as default\n",
    "\n",
    "model1=LogisticRegression(penalty='l2', C=1.0)\n",
    "model1.fit(X_train, Y_train)\n",
    "predictions1=model1.predict(X_test)\n",
    "cnf_matrix=confusion_matrix(Y_test, predictions1)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1675,   48],\n",
       "       [ 338,   84]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And you can see the result is the same, because this is what we ran the first time anyway without\n",
    "# inputting any values as these are the default. If we change the hyperparameter C we can change the fit\n",
    "\n",
    "model2=LogisticRegression(C=100)\n",
    "model2.fit(X_train, Y_train)\n",
    "predictions2=model2.predict(X_test)\n",
    "cnf_matrix2=confusion_matrix(Y_test, predictions2)\n",
    "cnf_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814452214452\n",
      "0.820046620047\n"
     ]
    }
   ],
   "source": [
    "# Below is the accuracy, by which sklearn is generally optimising when running a cross validation\n",
    "# (though check for any particular model what 'score' returns by looking at the documentation)\n",
    "\n",
    "print(model1.score(X_test, Y_test))\n",
    "print(model2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-04,   3.72759372e-04,   1.38949549e-03,\n",
       "         5.17947468e-03,   1.93069773e-02,   7.19685673e-02,\n",
       "         2.68269580e-01,   1.00000000e+00,   3.72759372e+00,\n",
       "         1.38949549e+01,   5.17947468e+01,   1.93069773e+02,\n",
       "         7.19685673e+02,   2.68269580e+03,   1.00000000e+04])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like the second C was better than the first, so I guess we should tune this!\n",
    "# Ok, all these things to tune is getting exhausting! Well, that's actually what being a data scientist\n",
    "# is about :) But don't worry, sklearn can at least make this easier with grid search to iterate over\n",
    "# different C values to test which gives the best cross validation accuracy results. That sounds good to me!\n",
    "# What is the liblinear? This is the solver used for maximising the log likelihood, we state it explicitly\n",
    "# here to ensure consistent results (generally you can trust sklearn to use the most computationally\n",
    "# optimal solving algorithm)\n",
    "\n",
    "logreg_cv1 = LogisticRegressionCV(Cs=15, cv=5, random_state=5, solver='liblinear')\n",
    "logreg_cv1.fit(X_train, Y_train)\n",
    "logreg_cv1.Cs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8949549437\n"
     ]
    }
   ],
   "source": [
    "# So here when we input Cs=15, we are saying we want a logarithmic range of 15 different Cs spanning\n",
    "# the range from 1e-4 to 1e4, and if we call the .Cs_ we output the Cs that were actually used\n",
    "# in the CV cycle of fittings. But which was the best one? The one which best fits the scoring metrics\n",
    "# out of the ones tested is called with .C_ (in this case, we used the accuracy score)\n",
    "\n",
    "best_C=logreg_cv1.C_\n",
    "best_C=float(best_C)\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: array([[ 0.80367394,  0.80367394,  0.80367394,  0.80367394,  0.80482204,\n",
       "          0.80137773,  0.80022962,  0.81056257,  0.80826636,  0.80941447,\n",
       "          0.80941447,  0.80941447,  0.80941447,  0.80941447,  0.80941447],\n",
       "        [ 0.80367394,  0.80367394,  0.80367394,  0.80367394,  0.80367394,\n",
       "          0.81171068,  0.81745121,  0.82319173,  0.81974742,  0.82204363,\n",
       "          0.82319173,  0.82319173,  0.82319173,  0.82319173,  0.82319173],\n",
       "        [ 0.80344828,  0.80344828,  0.80344828,  0.80344828,  0.80344828,\n",
       "          0.8045977 ,  0.81494253,  0.81609195,  0.8183908 ,  0.81954023,\n",
       "          0.81494253,  0.8137931 ,  0.81149425,  0.81149425,  0.81149425],\n",
       "        [ 0.80344828,  0.80344828,  0.80344828,  0.80344828,  0.80229885,\n",
       "          0.80114943,  0.79885057,  0.80344828,  0.8045977 ,  0.80574713,\n",
       "          0.80574713,  0.80574713,  0.80574713,  0.80574713,  0.80574713],\n",
       "        [ 0.80344828,  0.80344828,  0.80344828,  0.80344828,  0.80229885,\n",
       "          0.80229885,  0.81149425,  0.80689655,  0.81264368,  0.8091954 ,\n",
       "          0.80804598,  0.80804598,  0.80804598,  0.80804598,  0.80804598]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can return the scores with this, they were all pretty similar in this case\n",
    "\n",
    "logreg_cv1.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1675,   48],\n",
       "       [ 338,   84]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we can run our logistic regression model with the C parameter we decided upon\n",
    "\n",
    "model3=LogisticRegression(C=best_C)\n",
    "model3.fit(X_train, Y_train)\n",
    "predictions3=model3.predict(X_test)\n",
    "cnf_matrix3=confusion_matrix(Y_test, predictions3)\n",
    "cnf_matrix3\n",
    "plot_confusion_matrix(cnf_matrix3, classes=model3.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820046620047\n"
     ]
    }
   ],
   "source": [
    "# So this is our best accuracy\n",
    "print(model3.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have some idea about this process let's also investigate for a multinomial logistic regression.\n",
    "\n",
    "Previously we have introduced logistic regression as a format to return a probability for being in one of two classes\n",
    "(e.g. defaulted on loan or not, passed test or not). However, we can also use logistic regression for multiclass problems. This is multinomial logistic regression (as opposed to binomial logistic regression). An example might\n",
    "be whether someone will buy one of five different products from your website, depending on their browsing habits and geolocation.\n",
    "\n",
    "In this case, we consider separate logistic fits for each outcome (actually, each outcome minus one) and then compare the results with each other (all within one model implementation).\n",
    "\n",
    "For this to work there are some assumptions, such as that only one outcome is valid (i.e. outcomes should be mutually exclusive). They are also complete, in that there isn't another possible outcome (or rather, our model will miss this information if it is not included in the fit).\n",
    "\n",
    "You can fit this in such a way that each possible outcome is fit against all others (one-vs-rest), and this is then cycled through for each of the possible outcomes (actually the total number of outcomes minus one, and you calculate the last probability by just seeing what the remaining probability is compared to a total of 1). Hence rather than fitting an outcome vs no outcome binomial case, you are fitting for an outcome vs all other outcomes multinomial case (note that it's also possible to implement a separate multinomial classifier algorithm, but we need not get into this distinction).\n",
    "\n",
    "Hence, with each cycle through, you find the probability of one outcome (so that's your 1 in the binary case) compared to any other outcome (so that's your 0). You then go through the other cases until (if there are n outcomes) you have fit n-1 cases, and for the last you compare the sum of the previous probabilities to 1 so that in each case the total probability sums to 1 - meaning you have covered all possible outcomes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get another dataset from our favourite site, kaggle\n",
    "# This one is https://www.kaggle.com/c/sf-crime/data\n",
    "# Please download and put in the same directory as this file\n",
    "\n",
    "sf_crime = pd.read_csv(\"train.csv.zip\")\n",
    "sf_crime = sf_crime.dropna()\n",
    "sf_crime['Dates'] = pd.to_datetime(sf_crime.Dates)\n",
    "sf_crime['hour'] = sf_crime['Dates'].dt.hour\n",
    "sf_crime['month'] = sf_crime['Dates'].dt.month\n",
    "sf_crime['year'] = sf_crime['Dates'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories_to_use = ['VEHICLE THEFT','BURGLARY','DRUG/NARCOTIC']\n",
    "sf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(categories_to_use))]\n",
    "sf_crime_sub=sf_crime_sub.loc[0:200000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset out these columns as predictors\n",
    "subset=sf_crime_sub[[\"hour\", \"DayOfWeek\", \"PdDistrict\"]]\n",
    "\n",
    "# create dummy variables for them\n",
    "X=pd.get_dummies(subset, columns=[\"hour\", \"DayOfWeek\", \"PdDistrict\"], drop_first=True)\n",
    "\n",
    "# separate out the outcome but in this case don't create dummy variables, sklearn can handle this\n",
    "# so this means we are looking to predict three categories\n",
    "y=sf_crime_sub[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_1</th>\n",
       "      <th>hour_2</th>\n",
       "      <th>hour_3</th>\n",
       "      <th>hour_4</th>\n",
       "      <th>hour_5</th>\n",
       "      <th>hour_6</th>\n",
       "      <th>hour_7</th>\n",
       "      <th>hour_8</th>\n",
       "      <th>hour_9</th>\n",
       "      <th>hour_10</th>\n",
       "      <th>hour_11</th>\n",
       "      <th>hour_12</th>\n",
       "      <th>hour_13</th>\n",
       "      <th>hour_14</th>\n",
       "      <th>hour_15</th>\n",
       "      <th>hour_16</th>\n",
       "      <th>hour_17</th>\n",
       "      <th>hour_18</th>\n",
       "      <th>hour_19</th>\n",
       "      <th>hour_20</th>\n",
       "      <th>hour_21</th>\n",
       "      <th>hour_22</th>\n",
       "      <th>hour_23</th>\n",
       "      <th>DayOfWeek_Monday</th>\n",
       "      <th>DayOfWeek_Saturday</th>\n",
       "      <th>DayOfWeek_Sunday</th>\n",
       "      <th>DayOfWeek_Thursday</th>\n",
       "      <th>DayOfWeek_Tuesday</th>\n",
       "      <th>DayOfWeek_Wednesday</th>\n",
       "      <th>PdDistrict_CENTRAL</th>\n",
       "      <th>PdDistrict_INGLESIDE</th>\n",
       "      <th>PdDistrict_MISSION</th>\n",
       "      <th>PdDistrict_NORTHERN</th>\n",
       "      <th>PdDistrict_PARK</th>\n",
       "      <th>PdDistrict_RICHMOND</th>\n",
       "      <th>PdDistrict_SOUTHERN</th>\n",
       "      <th>PdDistrict_TARAVAL</th>\n",
       "      <th>PdDistrict_TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour_1  hour_2  hour_3  hour_4  hour_5  hour_6  hour_7  hour_8  hour_9  \\\n",
       "6      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "46     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "49     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "59     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "    hour_10  hour_11  hour_12  hour_13  hour_14  hour_15  hour_16  hour_17  \\\n",
       "6       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "46      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "49      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "59      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "    hour_18  hour_19  hour_20  hour_21  hour_22  hour_23  DayOfWeek_Monday  \\\n",
       "6       0.0      0.0      0.0      0.0      0.0      1.0               0.0   \n",
       "7       0.0      0.0      0.0      0.0      0.0      1.0               0.0   \n",
       "46      0.0      0.0      1.0      0.0      0.0      0.0               0.0   \n",
       "49      0.0      1.0      0.0      0.0      0.0      0.0               0.0   \n",
       "59      0.0      1.0      0.0      0.0      0.0      0.0               0.0   \n",
       "\n",
       "    DayOfWeek_Saturday  DayOfWeek_Sunday  DayOfWeek_Thursday  \\\n",
       "6                  0.0               0.0                 0.0   \n",
       "7                  0.0               0.0                 0.0   \n",
       "46                 0.0               0.0                 0.0   \n",
       "49                 0.0               0.0                 0.0   \n",
       "59                 0.0               0.0                 0.0   \n",
       "\n",
       "    DayOfWeek_Tuesday  DayOfWeek_Wednesday  PdDistrict_CENTRAL  \\\n",
       "6                 0.0                  1.0                 0.0   \n",
       "7                 0.0                  1.0                 0.0   \n",
       "46                0.0                  1.0                 0.0   \n",
       "49                0.0                  1.0                 0.0   \n",
       "59                0.0                  1.0                 1.0   \n",
       "\n",
       "    PdDistrict_INGLESIDE  PdDistrict_MISSION  PdDistrict_NORTHERN  \\\n",
       "6                    1.0                 0.0                  0.0   \n",
       "7                    0.0                 0.0                  0.0   \n",
       "46                   1.0                 0.0                  0.0   \n",
       "49                   0.0                 0.0                  0.0   \n",
       "59                   0.0                 0.0                  0.0   \n",
       "\n",
       "    PdDistrict_PARK  PdDistrict_RICHMOND  PdDistrict_SOUTHERN  \\\n",
       "6               0.0                  0.0                  0.0   \n",
       "7               0.0                  0.0                  0.0   \n",
       "46              0.0                  0.0                  0.0   \n",
       "49              1.0                  0.0                  0.0   \n",
       "59              0.0                  0.0                  0.0   \n",
       "\n",
       "    PdDistrict_TARAVAL  PdDistrict_TENDERLOIN  \n",
       "6                  0.0                    0.0  \n",
       "7                  0.0                    0.0  \n",
       "46                 0.0                    0.0  \n",
       "49                 0.0                    0.0  \n",
       "59                 0.0                    0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VEHICLE THEFT    9138\n",
       "BURGLARY         8161\n",
       "DRUG/NARCOTIC    7788\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train test split with a stratify term on the outcome to keep datasets balanced on the three outcomes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg=LogisticRegression(penalty='l2', C=1.0)\n",
    "logreg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BURGLARY', 'DRUG/NARCOTIC', 'VEHICLE THEFT'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We output the three classes so we can read the numpy array of\n",
    "# probabilities, these will be in the same order\n",
    "\n",
    "logreg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13500752,  0.80834286,  0.05664962],\n",
       "       [ 0.43558074,  0.13823595,  0.42618331],\n",
       "       [ 0.31707555,  0.32953778,  0.35338667],\n",
       "       [ 0.24962164,  0.40022098,  0.35015739],\n",
       "       [ 0.34014617,  0.07196238,  0.58789146],\n",
       "       [ 0.38205162,  0.10921418,  0.5087342 ],\n",
       "       [ 0.27607142,  0.37025253,  0.35367606],\n",
       "       [ 0.15322909,  0.8130883 ,  0.0336826 ],\n",
       "       [ 0.45800788,  0.14266948,  0.39932263],\n",
       "       [ 0.09517009,  0.84489178,  0.05993813]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ok, so you can see for these cases which of the three classes has the\n",
    "# highest probability, that will be the one selected by the model when you\n",
    "# call the predict function (which tells you the most likely outcome)\n",
    "\n",
    "y_pred=logreg.predict_proba(X_test)\n",
    "y_pred[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DRUG/NARCOTIC', 'BURGLARY', 'VEHICLE THEFT', 'DRUG/NARCOTIC',\n",
       "       'VEHICLE THEFT', 'VEHICLE THEFT', 'DRUG/NARCOTIC', 'DRUG/NARCOTIC',\n",
       "       'BURGLARY', 'DRUG/NARCOTIC'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So compare this output to the above, you can see that in each\n",
    "# of the cases the class label corresponds to the column which had\n",
    "# the highest probability assigned\n",
    "\n",
    "y_pred2=logreg.predict(X_test)\n",
    "y_pred2[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=15, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring='accuracy', solver='liblinear', tol=0.0001,\n",
       "           verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great, so we ran logistic regression in a case where we had three possible outcomes\n",
    "# instead of just two. Now let's have a look at running grid search to test for the optimal C\n",
    "# value (C = inverse of regularisation strength)\n",
    "\n",
    "logreg_cv = LogisticRegressionCV(Cs=15, cv=5, solver='liblinear', scoring='accuracy')\n",
    "logreg_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for class:\n",
      "{'BURGLARY': 1.0, 'VEHICLE THEFT': 0.26826957952797248, 'DRUG/NARCOTIC': 0.071968567300115138}\n"
     ]
    }
   ],
   "source": [
    "# find best C per class\n",
    "print('best C for class:')\n",
    "best_C = {logreg_cv.classes_[i]:x for i, (x, c) in enumerate(zip(logreg_cv.C_, logreg_cv.classes_))}\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.26826957952797248, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit regular logit model to 'DRUG/NARCOTIC' and 'BURGLARY' classes\n",
    "# use ridge penalty\n",
    "logreg_1 = LogisticRegression(C=best_C['BURGLARY'], solver='liblinear')\n",
    "logreg_2 = LogisticRegression(C=best_C['DRUG/NARCOTIC'], solver='liblinear')\n",
    "logreg_3 = LogisticRegression(C=best_C['VEHICLE THEFT'], solver='liblinear')\n",
    "logreg_1.fit(X_train, Y_train)\n",
    "logreg_2.fit(X_train, Y_train)\n",
    "logreg_3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BURGLARY': array([[ 0.67469521,  0.67469521,  0.67469521,  0.67469521,  0.68480523,\n",
       "          0.6907523 ,  0.68926554,  0.68986024,  0.69045495,  0.69045495,\n",
       "          0.69045495,  0.69045495,  0.69045495,  0.69045495,  0.69045495],\n",
       "        [ 0.67469521,  0.67469521,  0.67469521,  0.67647933,  0.68867083,\n",
       "          0.68748142,  0.68777877,  0.68807612,  0.68807612,  0.68837348,\n",
       "          0.68837348,  0.68837348,  0.68837348,  0.68837348,  0.68837348],\n",
       "        [ 0.67459845,  0.67459845,  0.67459845,  0.67430101,  0.68262939,\n",
       "          0.6876859 ,  0.68679358,  0.68679358,  0.68679358,  0.68679358,\n",
       "          0.68679358,  0.68679358,  0.68679358,  0.68679358,  0.68679358],\n",
       "        [ 0.67470238,  0.67470238,  0.67470238,  0.675     ,  0.67738095,\n",
       "          0.67946429,  0.67916667,  0.67857143,  0.67857143,  0.67857143,\n",
       "          0.67827381,  0.67827381,  0.67827381,  0.67827381,  0.67827381],\n",
       "        [ 0.67470238,  0.67470238,  0.67470238,  0.67529762,  0.68095238,\n",
       "          0.6860119 ,  0.68928571,  0.68928571,  0.68839286,  0.68839286,\n",
       "          0.68839286,  0.68839286,  0.68839286,  0.68839286,  0.68839286]]),\n",
       " 'DRUG/NARCOTIC': array([[ 0.68956289,  0.68956289,  0.70770146,  0.74695213,  0.75260184,\n",
       "          0.75498067,  0.75587273,  0.75527803,  0.75527803,  0.75527803,\n",
       "          0.75527803,  0.75527803,  0.75527803,  0.75527803,  0.75527803],\n",
       "        [ 0.68956289,  0.68956289,  0.7068094 ,  0.74754683,  0.74635742,\n",
       "          0.75111508,  0.75141243,  0.75141243,  0.75141243,  0.75141243,\n",
       "          0.75141243,  0.75141243,  0.75141243,  0.75141243,  0.75141243],\n",
       "        [ 0.68947055,  0.68947055,  0.70969661,  0.74449732,  0.74568709,\n",
       "          0.74895895,  0.74866151,  0.74776919,  0.74776919,  0.74776919,\n",
       "          0.74776919,  0.74776919,  0.74776919,  0.74776919,  0.74776919],\n",
       "        [ 0.68958333,  0.68958333,  0.70297619,  0.7485119 ,  0.75119048,\n",
       "          0.75714286,  0.75446429,  0.75386905,  0.75386905,  0.75416667,\n",
       "          0.75416667,  0.75416667,  0.75416667,  0.75416667,  0.75416667],\n",
       "        [ 0.68958333,  0.68958333,  0.70684524,  0.7547619 ,  0.75595238,\n",
       "          0.75654762,  0.7547619 ,  0.75446429,  0.75446429,  0.75446429,\n",
       "          0.75446429,  0.75446429,  0.75446429,  0.75446429,  0.75446429]]),\n",
       " 'VEHICLE THEFT': array([[ 0.6357419 ,  0.6357419 ,  0.6402022 ,  0.66250372,  0.66785608,\n",
       "          0.67201903,  0.67172168,  0.67172168,  0.67172168,  0.67023491,\n",
       "          0.67023491,  0.67023491,  0.67023491,  0.67023491,  0.67023491],\n",
       "        [ 0.6357419 ,  0.6357419 ,  0.63931014,  0.66190901,  0.67082962,\n",
       "          0.67975022,  0.68212905,  0.68034493,  0.68093964,  0.68123699,\n",
       "          0.68064228,  0.68064228,  0.68064228,  0.68064228,  0.68064228],\n",
       "        [ 0.63593099,  0.63593099,  0.64336704,  0.66835217,  0.66983938,\n",
       "          0.67073171,  0.67370613,  0.6748959 ,  0.6748959 ,  0.67400357,\n",
       "          0.67400357,  0.67400357,  0.67400357,  0.67400357,  0.67400357],\n",
       "        [ 0.63571429,  0.63571429,  0.6389881 ,  0.67232143,  0.6764881 ,\n",
       "          0.6797619 ,  0.68005952,  0.68035714,  0.68125   ,  0.68095238,\n",
       "          0.68095238,  0.68095238,  0.68095238,  0.68095238,  0.68095238],\n",
       "        [ 0.63571429,  0.63571429,  0.64166667,  0.66964286,  0.66904762,\n",
       "          0.67470238,  0.67380952,  0.67172619,  0.67142857,  0.67142857,\n",
       "          0.67142857,  0.67142857,  0.67142857,  0.67142857,  0.67142857]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_cv.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1072  507 1114]\n",
      " [ 494 1277  799]\n",
      " [ 647  411 1958]]\n",
      "[[1087  494 1112]\n",
      " [ 509 1270  791]\n",
      " [ 656  418 1942]]\n",
      "[[1084  501 1108]\n",
      " [ 502 1275  793]\n",
      " [ 652  409 1955]]\n"
     ]
    }
   ],
   "source": [
    "# predict and build confusion matrices for the models above\n",
    "Y_1_pred = logreg_1.predict(X_test)\n",
    "Y_2_pred = logreg_2.predict(X_test)\n",
    "Y_3_pred = logreg_3.predict(X_test)\n",
    "\n",
    "conmat_1 = confusion_matrix(Y_test, Y_1_pred, labels=logreg_1.classes_)\n",
    "conmat_2 = confusion_matrix(Y_test, Y_2_pred, labels=logreg_2.classes_)\n",
    "conmat_3 = confusion_matrix(Y_test, Y_3_pred, labels=logreg_3.classes_)\n",
    "\n",
    "print(conmat_1)\n",
    "print(conmat_2)\n",
    "print(conmat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BURGLARY       0.48      0.40      0.44      2693\n",
      "DRUG/NARCOTIC       0.58      0.50      0.54      2570\n",
      "VEHICLE THEFT       0.51      0.65      0.57      3016\n",
      "\n",
      "  avg / total       0.52      0.52      0.52      8279\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BURGLARY       0.48      0.40      0.44      2693\n",
      "DRUG/NARCOTIC       0.58      0.49      0.53      2570\n",
      "VEHICLE THEFT       0.51      0.64      0.57      3016\n",
      "\n",
      "  avg / total       0.52      0.52      0.52      8279\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BURGLARY       0.48      0.40      0.44      2693\n",
      "DRUG/NARCOTIC       0.58      0.50      0.54      2570\n",
      "VEHICLE THEFT       0.51      0.65      0.57      3016\n",
      "\n",
      "  avg / total       0.52      0.52      0.52      8279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports\n",
    "print(classification_report(Y_test, Y_1_pred))\n",
    "print(classification_report(Y_test, Y_2_pred))\n",
    "print(classification_report(Y_test, Y_3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which C to use in this case, it may depend on our problem if we wish to give\n",
    "the best performance to one of our classes for example (e.g. that is the high end product we really\n",
    "want to predict if our customers will buy). Or perhaps we would need to use some average of the scores in each case\n",
    "but we would have to do this manually to see which is the best C. \n",
    "\n",
    "Furthermore we glossed over slightly whether\n",
    "to use l1 or l2 penalties. This may also depend on the extent to which you wish to see certain outcomes (for\n",
    "example, as previously discussed the lasso l1 penalty will return some coefficients as zero, so can be used for\n",
    "automated feature selection - which is useful if you have very large numbers of possible features).\n",
    "\n",
    "So instead, we can use sklearn's GridSearchCV function to do this for us. This abstracts away all those decisions\n",
    "and finds the best overall performance across all classes. So which approach to use depends on your\n",
    "particular problem and the extent to which you want to dive deep into it. But GridSearchCV can iteratively try\n",
    "a lot of different approaches and then tell you which one performed the best. So it does lots of the hard work for you. Doesn't that sound great!\n",
    "\n",
    "We can actually search through any of the parameters in the LogisticRegression() model that we want to test with this function, and we can of course use it for other fitting algorithms as well. So this is very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   25.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will take a bit of time (should be < 1 min)\n",
    "# Go through this and check it makes sense\n",
    "# Check the documentation, there are many possible scorings you can use\n",
    "\n",
    "logreg_f = LogisticRegression()\n",
    "C_vals = [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs = GridSearchCV(logreg_f, {'penalty':penalties, 'C':C_vals}, verbose=True, cv=5, scoring='accuracy')\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the parameters we tested, we could also have tested others too\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHWCAYAAABZiKJMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a74a290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BURGLARY       0.48      0.40      0.43      2693\n",
      "DRUG/NARCOTIC       0.58      0.49      0.53      2570\n",
      "VEHICLE THEFT       0.50      0.65      0.57      3016\n",
      "\n",
      "  avg / total       0.52      0.52      0.51      8279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gs_logreg = LogisticRegression(C=gs.best_params_[\"C\"], penalty=gs.best_params_[\"penalty\"], solver='liblinear')\n",
    "gs_logreg.fit(X_train, Y_train)\n",
    "Y_ = gs_logreg.predict(X_test)\n",
    "conmat_gs_logreg = confusion_matrix(Y_test, Y_, labels=gs_logreg.classes_)\n",
    "plot_confusion_matrix(conmat_gs_logreg, classes=gs_logreg.classes_)\n",
    "print(classification_report(Y_test, Y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51709143616378794"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logreg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our best model in this case! Try playing around, and perhaps adding in new features and running through the\n",
    "process again using GridSearchCV so you don't have to take so many steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
