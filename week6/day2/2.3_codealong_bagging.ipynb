{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Ensemble Methods - Decision Trees and Bagging\n",
    "Week 6| Lesson 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Explain the power of using ensemble classifiers\n",
    "- Know the difference between a base classifier and an ensemble classifier\n",
    "- Describe how bagging works\n",
    "- Use the bagging classifier in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- perform a classification\n",
    "- use label encoder\n",
    "- use cross validation to evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening\n",
    "In the previous lectures we learned about Decision Trees, APIs and SQL Joins: many different techniques that combined together give us powerful tools to analyze and process data. Today we will learn about ensemble techinques: ways to combine different models in order to obtain a more powerful model.\n",
    "\n",
    "Before we dive into the subject, let's recap a few things learned so far:\n",
    "\n",
    "**Check:** What classifiers have we learned about so far? Which one is your favourite?\n",
    "\n",
    "**Check:** How did we assess the \"goodness\" of a particular model?\n",
    "\n",
    "**Check:** Any idea on how we could improve the performance of a model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: Ensemble Techniques\n",
    "\n",
    "\n",
    "### Ensembles\n",
    "Ensemble techniques are supervised learning methods to improve predictive accuracy by combining several base models in order to enlarge the space of possible hypotheses to represent our data. Ensembles are often much more accurate than the base classifiers that compose them.\n",
    "\n",
    "**Check:** Short discussion:\n",
    "- when this might be useful? \n",
    "- can you think of a business case where we may want to get a very very accurate model (despite it being more complex)\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "- In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator will often then perform better than any of the single base estimators.\n",
    "\n",
    "Examples of this family include Bagging methods and Random Forests.\n",
    "\n",
    "- The other family of ensemble methods are boosting methods, where base estimators are built sequentially. The motivation is to combine several weaker models to produce a more powerful ensemble. We will discuss these in a future lecture.\n",
    "\n",
    "Examples of this family include AdaBoost and Gradient Tree Boosting.\n",
    "\n",
    "\n",
    "![Ensemble](./assets/images/Ensemble.png)\n",
    "\n",
    "\n",
    "### The Hypothesis space\n",
    "\n",
    "In any supervised learning task, our goal is to make predictions of the true classification function $f$ by learning the classifier $h$. In other words we are searching in a certain hypothesis space for the most appropriate function to describe the relationship between our features and the target.\n",
    "\n",
    "**Check:** Can you give an example of how this search is performed using one of the classifiers you know?\n",
    "\n",
    "**Check:** What reasons could be preventing our hypothesis reaching perfect score?\n",
    "\n",
    "There could be several reasons why a base classifier doesn't perform terribly well in trying to approximate the true classification function. We can call the three types:\n",
    "- statistical problem\n",
    "- computational problem\n",
    "- representational problem\n",
    "\n",
    "\n",
    "#### The statistical problem\n",
    "If the amount of training data available is small, the base classifier will have difficulty converging to $h$.\n",
    "\n",
    "An ensemble classifier can mitigate this problem by \"averaging out\" base classifier predictions to improve convergence. This can be pictorially represented as a search in a space where multiple partial perspectives are combined to obtain a better picture of the goal.\n",
    "\n",
    "![Statistical Problem](./assets/images/statistical.png)\n",
    "\n",
    "(source: http://www.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-ensemble-00.pdf)\n",
    "\n",
    "The true function $f$ is best approximated as an average of the base classifiers.\n",
    "\n",
    "**Check:** How would you explain the statistical problem?\n",
    "\n",
    "#### The computational Problem\n",
    "Even with sufficient training data, it may still be computationally difficult to find the best classifier $h$.\n",
    "\n",
    "For example, if our base classifier is a decision tree, an exhaustive search of the hypothesis space of all possible classifiers is extremely complex.\n",
    "\n",
    "In fact this is why we use a heuristic algorithm (greedy search).\n",
    "\n",
    "An ensemble composed of several _Base Classifiers_ with different starting points can provide a better approximation to $f$ than any individual _Base Classifiers_.\n",
    "\n",
    "![Computational Problem](./assets/images/computational.png)\n",
    "\n",
    "The true function $f$ is often best approximated by using several starting points to explore the hypothesis space.\n",
    "\n",
    "**Check:** How would you explain the computational problem?\n",
    "\n",
    "#### The representational Problem\n",
    "Sometimes $f$ cannot be expressed in terms of our hypothesis at all. To illustrate this, suppose we use a decision tree as our base classifier. A decision tree works by forming a rectilinear partition of the feature space, i.e it always cuts at a fixed value along a feature.\n",
    "\n",
    "![Decision Tree boundary](./assets/images/dtcut.png)\n",
    "\n",
    "But what if $f$ is a diagonal line?\n",
    "\n",
    "Then it cannot be represented by finite rectilinear segments, so the 'true' decision boundary cannot be obtained by a decision tree classifier.\n",
    "\n",
    "However, it may be still be possible to approximate $f$ or even to expand the space of representable functions using ensemble methods.\n",
    "\n",
    "![Representational Problem](./assets/images/representational.png)\n",
    "\n",
    "**Check:** How would you explain the representational problem?\n",
    "\n",
    "### Characteristics of Ensemble methods\n",
    "\n",
    "In order for an ensemble classifier to outperform a single base classifier, the following conditions must be met:\n",
    "\n",
    "- **accuracy**: base classifiers outperform random guessing\n",
    "- **diversity**: misclassifications must occur on different training examples\n",
    "\n",
    "\n",
    "![Ensemble performance](./assets/images/ensemble_performance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Find real world uses of ensemble methods\n",
    "\n",
    "There are many examples of real-world uses of ensemble methods, the most famous one probably being the Netflix prize.\n",
    "\n",
    "In small groups of 3-4 people look for one or two real world applications of ensemble methods and then choose your favourite one to report to the class.\n",
    "\n",
    "Here are some sources to start your search:\n",
    "\n",
    "- [Kaggle competitions](https://www.kaggle.com/competitions)\n",
    "- [Review Paper](./assets/papers/classifier_ensembles_review.pdf)\n",
    "- [KDNuggets article 1](http://www.kdnuggets.com/2016/02/ensemble-methods-techniques-produce-improved-machine-learning.html)\n",
    "- [KDNuggets article 2](http://www.kdnuggets.com/faq/simple-data-mining-case-study.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction_2\"></a>\n",
    "## Introduction: Bagging\n",
    "\n",
    "_Bootstrapping_ is a general process by which we take a subset of our data _with replacement_, so rather than something we have seen before with methods like cross validation where we clearly divide our data set into the train\n",
    "and the cross validation and we do not mix the two - which is subsetting the data _without replacement_ - in the case\n",
    "of bootstrapping we reach into the pool of possible datapoints and take one out, then when we reach in again to pull\n",
    "out the next data point we could potentially pick the same data point as we already pulled out. This is a random\n",
    "process whether a data point is repeated or not, but given the probability of being chosen we expect around 2/3 of the data to be chosen in each bag and 1/3 are likely to be repeated picks if we take a sample which is the same size as the original dataset. We can then\n",
    "compare variation in the statistics such as mean and variance amongst the different bootstrap samples as a way of drawing conclusions around our dataset and so gain more statistical insight with a smaller number of datapoints than simply our total mean and variance as our only two statistics.\n",
    "\n",
    "_Bagging (Bootstrap AGGregatING)_ is a method that involves fitting multiple classifiers/regressions by resampling the dataset using bootstrapping. We learn $k$ base classifiers on $k$ different samples of training data.  These samples are independently created by resampling the training data using uniform weights (eg, a uniform sampling distribution). In other words each model in the ensemble votes with equal weight (unlike other methods such as boosting we look at tomorrow). In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. \n",
    "\n",
    "|Original|1|2|3|4|5|6|7|8|\n",
    "|----|----|----|----|----|----|----|----|----|\n",
    "|Training set 1|2|7|8|3|7|6|3|1|\n",
    "|Training set 2|7|8|5|6|4|2|7|1|\n",
    "|Training set 3|3|6|2|7|5|6|2|2|\n",
    "|Training set 4|4|5|1|4|6|4|3|8|\n",
    "\n",
    "Given a standard training set $D$ of size $n$, bagging generates $m$ new training sets $D_i$, each of size $nâ€²$ (which can be equal to $n$), by sampling from $D$ uniformly. By sampling with replacement, some observations will be repeated in each $D_i$. The $m$ models are fitted using the above $m$ samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "\n",
    "**Check:** Can you propose another sample to add to those above? Call out the numbers you would include.\n",
    "\n",
    "Bagging reduces the variance in our generalisation error by aggregating multiple base classifiers together (provided they satisfy our earlier requirements). Since each sample of training data is equally likely, bagging is not very susceptible to overfitting with noisy data. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees). Boosting is discussed tomorrow and involves refitting the model with a bag that is weighted towards misclassified data points in the prior pass, and then refitting and passing over again and again. Random Forests, certainly the most popular ensemble decision tree method, is then another iteration on this with random subselecting of predictors (I am deliberately not going into this too much since it is discussed tomorrow, just providing some context in which to place the bagging approach).\n",
    "\n",
    "When it comes to reporting error rates with the bagging method, we do not have clear cut training and test sets. For this then various algorithms have been developed to come up with reasonable estimates of the generalisable test error by keeping track of the points collected in the bag and the points out of the bag, and through various averaging arriving at a reasonable estimate of the expected test error that is tolerable for most applications but obviously not as clear cut as the cross validation approach. I've provided a copy of a 1986 review paper which covers such approaches for those who are interested [here](./assets/papers/bootstrap_error_estimates.pdf). I've also provided a 1996 paper which describes bagging predictors as implemented in sklearn [here](./assets/papers/bagging_predictors.pdf) (i.e. the sklearn implementation was based on this paper).  Alternatively, we can simply calculate score based on our own cross validation / train test split and so we know we will not be using points that might be present in the bagged sets (this is the approach taken below), and this removes this element of concern though it means, of course, fewer points from which to select for the training bags.\n",
    "\n",
    "Bear in mind that bootstrapping and fitting multiple models based on a bagging approach is just one way to try to get as much from a limited dataset as possible. Another would be to assume the distribution is normal and randomly generate new data from a distribution with the same mean and standard deviation as the sample (such as the Monte Carlo simulation approach we saw last week). You could then fit multiple models on such datasets and average them in a similar way. Bootstrapping does not make assumptions about the underlying distribution of the data because it simply treats every point as equally likely to be selected (even if that point has been selected before). Typically we might fit several hundred models in the ensemble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Bagging Classifier in Scikit Learn\n",
    "In scikit-learn, bagging methods are offered as a unified `BaggingClassifier` meta-estimator (resp. BaggingRegressor), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms of samples and features), while bootstrap and bootstrap_features control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization error can be estimated with the out-of-bag samples by setting oob_score=True.\n",
    "\n",
    "As an example, we will compare the performance of a simple KNN classifier versus the Bagging Classifier on the car acceptability dataset, and the same for Decision Trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "df = pd.read_csv('assets/datasets/car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the features are categorical text we will need to encode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c != 'acceptability']\n",
    "X = df[features]\n",
    "X=pd.get_dummies(X, drop_first=True)\n",
    "y = df['acceptability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step is to calculate the cross_val_score on the two classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score:\t 0.726010994822\n",
      "Bagging Score:\t 0.738748541238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "bagging = BaggingClassifier(knn, n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"KNN Score:\\t\", cross_val_score(knn, X, y, cv=10).mean())\n",
    "print(\"Bagging Score:\\t\", cross_val_score(bagging, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees Score:\t 0.825490176186\n",
      "Bagging Score:\t 0.842831758123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "trees=DecisionTreeClassifier()\n",
    "bagging2=BaggingClassifier(trees, n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Trees Score:\\t\", cross_val_score(trees, X, y, cv=10).mean())\n",
    "print(\"Bagging Score:\\t\", cross_val_score(bagging2, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** Does bagging interfere with the cross validation? Are we leaking data and thus faking the cross val score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier details\n",
    "\n",
    "The `BaggingClassifier` meta-estimator has several parameters.\n",
    "\n",
    "**Check:** In pairs, look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) for a detailed description of each, and in particular find out what `max_samples` and `max_features` do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice\n",
    "\n",
    "Take a look at this car dataset and practice comparing the score of a base classifier with the bagging classifier. Explore also the effect of changing one or more parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation, test sets, and bootstrapping clarification\n",
    "\n",
    "We have something of a proliferation of sampling methods so let's go through them. We know that `train_test_split` allows us to split our data and perform model fitting on train data and report a score based on applying that fit to the test data, which more realistically tells us how our model is likely to perform against new data than testing against the training data. Hence we don't necessarily always apply such splits, it depends what we are reporting. In the case of reporting the error, it is a good idea because the error on the train set will be unrealistically low. In order to use more of the data points for fitting, we can do a cross validation method which splits our data into folds for us, performs a fit many times and reports the error on each fold (which was not used for the fitting), but still allows us to overall use all of the data points for fitting. Hence when we use `cross_val_score`, we report our best estimate of the accuracy score by using cross validation rather than using eg a train-test split. That's because we use all of our available data points for the fitting (overall) and we use all our available data points for the testing (overall). It also means we can use fewer lines of code since sklearn implements this for us. So there's a double win there by using that as our default method of reporting accuracy, rather than a `train_test_split`.\n",
    "\n",
    "When we perform a `GridSearchCV`, this is also using k folds cross validation (using the `cv` parameter this time, rather than `k` because it can actually take different arguments than just an integer if you wish to set it up this way - if this parameter is None it will default to 3 folds, if it is an integer it behaves the same way as `k`). That's required for this method (there is no grid search without cross validation implemented) so we report a realistic score for each element of the grid by fitting and testing across each fold for each parameter. Then the hyperparameter that reported to best averaged score across the folds of cross validation is returned as the optimal. In this case putting a cross validated algorithm such as `LogisticRegressionCV()` within the `GridSearchCV` would not serve an advantage as there would be an overproliferation of splits of the data in a redundant manner. \n",
    "\n",
    "One case we saw last week where there is an advantage of putting a cross validating operater with a `GridSearchCV` is the `RFECV`. That's because the `RFECV` is a case which actually has a different functional purpose to `RFE`, in that it returns an optimal number of features to keep - whereas for `RFE` there is no way to calculate this so the user must input the number of features to keep as a parameter. In `RFE` one does not report a score, since the aim is simply to rank features based on coefficient and retain the most important ones down to a user-specificied number. For `RFECV` we use the score from cross validation to determine how the iteratively formed models with different numbers of features actually perform and then return the number of features that returned the best score. So since this serves a different functional purpose to `RFE`, it does in that case make sense to use it within a `GridSearchCV`.\n",
    "\n",
    "For the bootstrap, note that this is simply a different method of sampling the data. We could, for example, take different folds of the data and fit our classification trees to each of those folds and average them. By bootstrapping we can more straightforwardly increase the number of bags we use to the hundreds, or even thousands, without limiting our model fits by having too few data points in each. Since we are not using this approach to directly report an error, it is not a problem that we re-use data points. In the end we are simply sampling the underlying population in differing ways. Bootstrapping would not make much sense for `train_test_split` since we would end up reporting a score for the test that included points that were in the training set, but for the bagging approach it allows us to have a greater number of splits of our data and indeed model performance often improves by this method (though we see modest improvement here, it can be dramatic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion \n",
    "\n",
    "Today we have learned about Ensemble Models and Bagging Classifier. We have learned how they improve the performance of individual base models thanks to their better ability to approximate the real prediction function in a supervised learning problem.\n",
    "\n",
    "**Check:** Which of the 3 problems does a bagging classifier solve?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Ensemble models on wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "- [Bagging on wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n",
    "- [Ensemble methods on Scikit Learn](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Bagging Classifier documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "- [Bias Varias Decomposition Scikit Learn Example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#example-ensemble-plot-bias-variance-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
