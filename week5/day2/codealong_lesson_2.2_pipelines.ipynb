{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Pipelines and custom transfomers in SKLearn\n",
    "Week 5 | Lesson 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- create pipelines for cleaning and manipulating data\n",
    "- use pipelines to preprocess data from the SQL database\n",
    "- use pipeline in combination with classification\n",
    "- create a custom transformer using the `TransformerMixin` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- extract data from a database\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many organizations rely on data engineering teams to encode these common tasks into pipelines. **Data pipelines** are a series of automated data transformations that ensure the validity of your work for routine data maintenance tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines \n",
    "\n",
    "The term _Pipeline_ is used to indicate a series of concatenated data transformations. Each stage of the pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end as water flows through... a pipeline.\n",
    "\n",
    "![Pipeline](./assets/images/pipeline.png)\n",
    "\n",
    "Each processing stage has an input, where data comes in, and an output, where processed data comes out.\n",
    "\n",
    "**Check:** What are some examples of data transformations?\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines in scikit-learn \n",
    "One way to improve coding and model management is to use pipelines in `scikit-learn`. These tie together all the steps that you may need to prepare your dataset and make your predictions. Because you will need to perform all of the same transformations on your evaluation data, putting this all together makes sense.\n",
    "\n",
    "To show how a pipeline works, we'll use an example involving Natural Language Processing - this is a topic we will get more into next week. \n",
    "\n",
    "The data comes from the [Evergreen Stumbleupon Kaggle Competition](https://www.kaggle.com/c/stumbleupon/data), which you have seen before. You will need to get the train data, which you presumably already have somewhere or you can redownload it from the link. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) are provided. We'll focus on the page title text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data['boilerplate'].map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data['boilerplate'].map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "titles = data['title'].fillna('')\n",
    "titles.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51332\n",
       "0    0.48668\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label']\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to predict evergreenness from this text data. Each datapoint is a string of free form text. How can we feed this to a model? The simplest way is to build a dictionary of words and use those as features. This is what a `CountVectorizer` does.\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "|Sentence|the|cat|is|on|table|blue|\n",
    "|---|---|---|---|---|---|---|\n",
    "|The cat is on the table|2|1|1|1|1|0|\n",
    "|The table is blue|1|0|1|0|1|1|\n",
    "|...|||||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'air',\n",
       " u'air breathing',\n",
       " u'breathing',\n",
       " u'calls',\n",
       " u'calls air',\n",
       " u'holographic',\n",
       " u'holographic calls',\n",
       " u'ibm',\n",
       " u'ibm sees',\n",
       " u'sees',\n",
       " u'sees holographic']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a count of words appearing in the inputted text string\n",
    "# by setting ngram_range to (1,2) we ask to return both single words and pairs of words\n",
    "# by setting binary=True we return a 0 if the word occurs and a 1 if it does occur, no matter how many times it occurs\n",
    "# by setting stop_words = 'english' we ignore very common words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, ngram_range=(1, 2), stop_words='english', binary=True)\n",
    "\n",
    "vectorizer.fit(['IBM Sees Holographic Calls Air Breathing'])\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having fit the vectorizer, if we now input a new string we return the existence or not of those\n",
    "# words / word pairs we set\n",
    "\n",
    "vectorizer.transform(['IBM Sees Holographic Air']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** What is the meaning of the various parameters used at initialisation of the CountVectorizer?\n",
    "\n",
    "\n",
    "Let's use the vectorizer to fit all the titles and build a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best',\n",
       " u'best new',\n",
       " u'better',\n",
       " u'betty',\n",
       " u'betty crocker',\n",
       " u'big',\n",
       " u'biggest',\n",
       " u'birthday',\n",
       " u'biscuits',\n",
       " u'bites',\n",
       " u'black',\n",
       " u'black bean',\n",
       " u'blog',\n",
       " u'blog archive',\n",
       " u'blogs',\n",
       " u'blood',\n",
       " u'blue',\n",
       " u'blueberry',\n",
       " u'body',\n",
       " u'boost']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use fit to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "vectorizer.get_feature_names()[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use tranform to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this input X, a matrix of all common n-grams in the dataset, as an input to our classifier. We want to classify how evergreen a story is, based on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.74695864  0.75578093  0.75608766]\n",
      "Average CVScore: 0.753 +/- 0.004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "# that's accuracy score for logistic regression\n",
    "scores = cross_val_score(model, X, y)\n",
    "\n",
    "print('CV scores: {}'.format(scores))\n",
    "print('Average CVScore: {:0.3f} +/- {:0.3f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Steps in Pipelines\n",
    "\n",
    "Pipelines combines many steps, both pre-processing and model building, into a _single object_. Rather than manually evaluating the transformers and then feeding them into the models, pipelines can tie both of these steps together.\n",
    "\n",
    "Similar to models and vectorizers in scikit-learn, pipelines are equipped with `fit` and `predict` or `predict_proba` methods (as any model would be), and they ensure that proper data transformations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set\n",
    "training_data = data[:6000]\n",
    "X_train = training_data['title'].fillna('')\n",
    "y_train = training_data['label']\n",
    "\n",
    "# These rows are rows obtained in the future, unavailable at training time\n",
    "X_new = data[6000:]['title'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We already set these above but just to be explicit we reinitialise them\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, ngram_range=(1, 2), stop_words='english', binary=True)\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# We input key, value pairs to the pipeline. The key is a name we choose, but the value must be the\n",
    "# object referring to the step of the pipeline, which is then fit\n",
    "\n",
    "pipeline = Pipeline([('vec', vectorizer), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=999, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say I want to set a hyperparameter of one of the elements of the pipeline\n",
    "# after I've initialised it. I can do this with a two underscore __ notation\n",
    "# invoking the key name that was set for the element (model or vec)\n",
    "\n",
    "pipeline.set_params(model__C=10) \n",
    "#pipeline.set_params(vec__max_features=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "       ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the full pipeline. This means we perform the steps laid out above\n",
    "# First we fit the vectorizer, and then feed the output of that into the fit function of the model\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52191125,  0.71543968,  0.98634554, ...,  0.7151964 ,\n",
       "        0.39140431,  0.32492484])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we apply each step of the pipeline for predictions\n",
    "# The text is transformed with the vectorizer to match the features from the pipeline\n",
    "# however instead of calling model.fit() as the last step the pipeline will call model.predict_proba()\n",
    "\n",
    "pipeline.predict_proba(X_new)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Add a `MaxAbsScaler` scaling step to the pipeline, which should occur after the vectorization.\n",
    "\n",
    "So to clarify, the advantage is calling .fit() and .predict() on the whole pipeline and only needing to call these steps once. You could also have made your own function that linked these together but sklearn can handle all the steps\n",
    "nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vec', CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=999, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'model__C': [0.1, 10, 100], 'vec__binary': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search on both CountVectorizer and LogisticRegression\n",
    "# that's pretty useful!\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = dict(vec__binary=[True, False], model__C=[0.1, 10, 100])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.1, 'vec__binary': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## make_pipeline and the preprocessing module\n",
    "\n",
    "Scikit-learn pipelines can also be built using the `make_pipeline` command, which has a simpler syntax where you don't need to pass a name (item gets given a standard name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), LogisticRegression())    \n",
    "\n",
    "pipe2 = Pipeline(steps=[('standard_scal',StandardScaler()), ('logistic_regr',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standard_scal', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logistic_regr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pipelines created above are identical, i.e. this is just an alternative way to do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in sklearn (in pairs)\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Check** in pairs, assign one function to each pair, read about it in the doc and then explain it to the class.\n",
    "\n",
    "\n",
    "Data Manipulators\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "- VarianceThreshold\n",
    "\n",
    "Data Imputation\n",
    "- Imputer\n",
    "\n",
    "Function Transformer\n",
    "- FunctionTransformer\n",
    "\n",
    "Label Manipulators\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer\n",
    "\n",
    "\n",
    "Ok, so we can implement several of these in a pipeline in order to preprocess our data. In fact this is what we will do in the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo_2\"></a>\n",
    "## Custom Transformers\n",
    "\n",
    "We can implement custom transformers by extending the BaseClass in Scikit-Learn. This will be necessary if we want the transformer to only act on certain columns. Thankfully it is not too complicated as we just need to define how it responds to transform and fit. This is something you would have to do for production level code, so you can get a chance to implement python classes as we saw last week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 4, 0, 0],\n",
       "       [0, 0, 6, 0],\n",
       "       [0, 0, 0, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# the inheritance of BaseEstimator and TransformerMixin allows use of methods such as fit_transform()\n",
    "# which are not explicitly defined here (you can just always import them for this purpose)\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # This initialises the object, any argument we want to define has to appear here\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    # This is where the actual transformation that we want occurs\n",
    "    def transform(self, X, *_):\n",
    "        return X * self.factor\n",
    "    \n",
    "    # This needs to be defined but we don't want it to do anything\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "fm = FeatureMultiplier(2)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "fm.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Compare this with the `FunctionTransformer` from the preprocessing module.\n",
    "\n",
    "**Check** Implement a custom transformer that selects a specific feature from a Pandas dataframe. It should be initialized with the column name or the column index and it should return the selected column when transforming a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General form for a custom transformer class\n",
    "\n",
    "class SampleExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vars_):\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        # some manipulation\n",
    "\n",
    "    def fit(self, X, *_):\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_union, FeatureUnion\n",
    "\n",
    "What if you want to have several pipelines, that perform manipulations on data in a step-wise fashion, and then recombine the results of these pipes back together? That's where the union logic will come in. You can have your overall process with several pipeline branches, and then these pipeline branches can act independently (so not in sequence, as the pipeline does) and then recombine at the end.\n",
    "\n",
    "Similarly to how we can either call Pipeline, or make_pipeline then we can call FeatureUnion or make_union to perform this operation.\n",
    "\n",
    "While scikit-learn pipelines help with managing the transformation from raw data, there may be many steps before this takes place in your pipeline. These pipelines are often referred to as _ETL pipelines_ for \"Extract, Transform, Load.\" In an _ETL pipeline_, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then \"loaded\" into whatever system or analysis requires them. We will see this in the lab. Other ways you can manage such ETL pipelines besides the sklearn tools include:\n",
    "- [Luigi](https://github.com/spotify/luigi), developed by Spotify\n",
    "- [Airflow](https://github.com/airbnb/airflow), developed by AirBnB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Putting it all together\n",
    "\n",
    "**Check** Revisit the dataset of lab 1.4. How could you use `make_pipeline` and `make_union` to build a pipeline that performs the same steps all in one pass?\n",
    "\n",
    "1. review lab 1.4 and identify the steps that were perfomed\n",
    "- for each of this steps figure out what the input and what the output is\n",
    "    - is the input the whole dataframe or only a subset of the features?\n",
    "    - is the output new features or a prediction?\n",
    "- for each of this steps idendify what kind of transformer is needed:\n",
    "    - is it a custom transformer?\n",
    "    - does scikit-learn provide a transformer like this out of the box?\n",
    "- if different features are treated differently, how to recombine them?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "Finally, one of the biggest advantages of the pipeline is the ability to perform gridsearch across the whole pipeline to tune hyperparameters of different steps. In this case we follow a notation in which we refer to the pipeline object by the name we gave it in the Pipeline step (or by its standard name in the make_pipeline method where we don't pass a dictionary key as a name) and add to underscores and the parameter (e.g. logisticregression__C) as the key of the\n",
    "hyperparameter dictionary, and the values are the ones we wish to search over (as before). Hence we just do the whole grid search over a single pipeline object. This single thing is one of the biggest reasons to use pipelines and unions, and we will see it in the lab (though right at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "We have learnt how to use the `Pipeline` construct in order to chain several instructions in one single class. This enables to treat data-processing from a more abstract and more powerful perspective, and it's a pre-cursor to the work we will do when working with Big Data technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Pipelines and Feature Union](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "- [Example with complex pipeline](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#example-hetero-feature-union-py)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
